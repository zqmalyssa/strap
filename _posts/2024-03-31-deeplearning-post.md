---
layout: post
title: deeplearning
tags: [deeplearning]
author-id: zqmalyssa
---

关于机器学习、深度学习的内容，基于limu的d2l去做一些总结

#### 基本概念

一个是环境的安装（python的下载，d2l的下载，jupyter的下载，book的下载，pycharm的下载等等）

learn内容：

- 读取一个csv文件，做预处理，均值，插值等等，然后变成pytorch的张量获取到 inputs 和 outputs
- torch的代码，如果矩阵赋值 b = a，那么如果b改变了，a也跟着改变的，是引用传递的感觉，想想java（只有值传递）
- 线性代数，sum哪个维度就是消除哪一维，但如果加上keepdmis，就是dimention变成1，但是维度的个数是不变的
- 矩阵计算，分子布局符号方式，偏导y标量/偏导x向量是行向量，偏导y向量/偏导x标量还是列向量，偏导y向量/偏导x向量就是矩阵了（结合前面的行列，y是(m,1)，x是(n, 1)那么出来的矩阵是(m, n)）
- 把上面的向量拓展到矩阵，矩阵偏导矩阵是一个四维张量，如(m, l)/(n, k)变成 (m, l, k, n)，矩阵（m, l）偏导向量(n, 1)得出来就是(m, l, n)，矩阵(m, l)偏导标量那么还是(m, l)
- 标量链式法则
- 向量链式法则， 在标量y / 向量x的前提下，如果u是个标量，y标量 / x向量是 (1, n)，如果u是向量，（1, n）= (1, k) (k, n)。在向量y / 向量x的条件下，（m, n）= (m, k)(k, n) 是个矩阵
- 自动求导计算一个函数在指定值上的导数，它有别于 符号求导 和 数值求导。将代码分解成操作子，计算表示成一个无环图
- 将代码分解成操作子，将计算表示成一个无环图，显示构造，也可以隐式的构造
- 自动求导的两种模式，结合链式法则，正向累积（从链式法则的内侧往外侧计算），还有反向累积、又称反向传递（从链式法则的外侧往内侧计算），所以前向执行的时候，要存储中间的结果，反向执行的时候（用到之前存储的中间值），可以去除不需要的枝。复杂度上看的，正向和反向的计算复杂度都是O(n)，n操作子个数，通常正向和反向的代价类似（forward 和 backward差不多）。内存复杂度是O(n)，因为需要存储正向的中间结果（特别耗GPU资源）。那么反向跟正向累积对比，O(n)计算复杂度用来计算一个变量的梯度（再扫一遍，说是不是经常用？？？？），O(1)内存复杂度
- 假设对 y = 2x转秩x 的一个标量 对 x列向量 求导
- 神经网络求梯度的时候是需要正着算一遍 然后再反着算一遍的
- 深度学习为什么通常对标量求导，而不是对向量或者矩阵，因为loss通常是个标量，多个loss分别反向的时候是需要累积梯度的
- 线性回归是有显示解的，就是能计算出来，有显示解的一般模型比较简单，线性回归可以看做是单层神经网络
- 梯度下降通过不断沿着反梯度方向更新参数求解，小批量随机梯度下降是深度学习默认的求解方法，两个重要的超参数是批量大小和学习率（如何选择）
- 操作子 softmax(o)，把输出转换成一个和为1 的概率，exp(o) 指数是可以将数值 > 0，这样就有真实的概率（只有某个类的y值是1，其余是0） 和 预测的概率（每个类的y_hat，一个概率），分类问题是有多个输出（不同类别的概率）
- 用交叉熵来衡量两个概率的区别，将它作为损失，其梯度（对某个oj求偏导）是真实概率和预测概率的区别，softmax(o) - y（这个公式在官网端是可以进行推导的，带入softmax），不关心非正确类的预测值，而是正确类的预测值，正确类的概率要足够大
- 损失函数loss，机器学习用来衡量真实值和预测值之间的区别，三个常见的损失函数，第一个，均方损失（看导数，导数的绝对值越大，说明变化越大，导数的绝对值越小，说明变化越小），第二个，绝对值损失（导数值一直比较平稳），第三个，结合的Huber函数就是一开始变换比较平稳，但是到最后（靠近真实值的时候）就会放缓。蓝色是本身的函数，黄色是导函数（都可导）[常见损失函数](https://www.bilibili.com/video/BV1K64y1Q7wu?spm_id_from=333.788.videopod.episodes&p=2)，蓝色就是y为0的时候，当变化y_hat，损失函数的变化值
- 逻辑回归（logistic regression）和 softmax回归（softmax regression）是两种用于分类问题的常见机器学习算法，逻辑回归用于二分类问题，softmax是多分类问题，逻辑回归相当于输出一个0-1的概率值就行了，另一个不就是 1 - 算出的值么，逻辑回归是softmax回归的一个特例
- 为什么用交叉熵，不用相对熵、互信息熵作为损失函数（因为交叉熵算起来简单）
- 这样的n分类问题，对于每一个类别来说，是不是可以认为只有1个正类，n-1个负类，会不会不平衡呢（会的，但是0，1的编码会去掉负类。要关心的是有没有足够多的负类）
- 统计是解释模型的一个工具，但是在后面的深度学习中，跟统计相关的不多，主要还是跟线性代数有关，最小化损失函数，就是最大化模型的似然函数
- 感知机（而分类模型，求解算法等价于使用批量大小为1的梯度下降算法，注意还不是随机梯度下降算法），人工智能最早最早的一个模型，cigema函数，x > 0 是1，否则是0，或者否则是 -1。二分类问题，-1 或 1，线性回归输出是实数，softmax回归输出的是概率。感知机不能拟合XOR函数，它只能产生线性分割面，感知机如果是一个二维的输入的话，其实就是平面上的一条线，在x/y平面里，对于XOR函数（xy正负相同为-1，xy正负相反为1），所以一条线是切不开这样的分布的。归根原因是只能产生线性的分割面（1969的时候发现的，Minsky & Papert）。AI的寒冬，建这么大机器，连XOR函数都无法处理，10年到15年后发现有办法做这个事情的，那就是多层感知机
- 多层感知机，解决XOR问题，也就是用两根线，，，有点强，最后给两根线的结果做一个product，先进入蓝色分类器，再进入黄色分类器，最后进入灰色分类器。隐藏层的大小是超参数，输入层的大小是不能改的，输入的维度是多大就是多大，输出的话还是多少类了，那么隐藏层的大小是可调节的
- 输入层、隐藏层、单个输出（这个就是最简单的单隐藏层单输出的神经网络），那么在到隐藏层的时候为什么需要一个【非线性的】激活函数，如果不加激活函数，那么把式子带进去，还是一个线性函数（也就是一个线性模型），仍然不解决XOR问题，所以激活函数一定要记得加！！！！
- 激活函数是有很多选择的，比如sigmoid函数，感知机的函数很硬，如果是sigmoid的话，会软一点，介于0-1。还有tanh这个激活函数，能投影到-1-1，还有ReLU的激活函数（rectified linear unit），max(x, 0)。因为 y = x肯定不行呗，还是线性，那就砍一刀，把下象限的给提上来，导数也是一边0，一边1，优势是算起来很快，不用做指数运算，指数运算是一件很贵的运算，一次指数运算可能等于cpu上的100次乘法运算，gpu可能好一点点，gpu有自己的单元做指数运算
- 加了一层隐藏层的多分类就是 多层感知机，而不加的话其实就是一个softmax多分类问题。有单隐藏层就可以做多隐藏层，最后一层到输出的一般不需要激活函数，激活函数就是保证层数不塌陷，所以又多了一个超参数就是隐藏层数
- 机器学习本身就是一种压缩的过程。。降维的过程，一张图片压缩压缩成一种分类，所以多隐藏层重要还是深度重要，要看的。最下面胖一点点没关系
- 训练误差（模型在训练数据上的误差） 和 泛华误差（模型在新数据集上的误差），当然模型需要在泛化误差中表现的更好。怎么计算训练误差和泛化误差？就是 验证数据集：一个用来评估模型好坏的数据集（例如拿出50%的训练数据，验证数据集不要跟训练数据混在一起）。测试数据集(test_dataset，不能用来调参的)：只用一次的数据集，例如，未来的考试，我出价房子的实际成交价，或用在Kaggle私有排行榜中的数据集。很多时候其实没有测试数据集（能只用一次的），那么就是用的验证数据集（validation data，也就是代码里写的很多的test_data）
- 实际情况确实没有足够多的数据集去使用，直接拿一半的数据集去验证有点伤，解决这个问题的方法是 K-则交叉验证。把训练数据集分割成K块，一个for，每次将第i块作为验证数据集，其余作为训练数据集，报告K个验证集误差的平均，常用的K 是5 或 10，如果是3折，至少每次用了66%的数据做训练。深度学习数据集一般会比较大，所以不怎么用K折法
- 过拟合（overfitting） 和 欠拟合（underfitting），模型容量，拟合各种函数的能力，低容量的模型难以拟合训练数据，高容量的模型可以记住所有的训练数据，模型足够大，然后用手段控制泛华误差往下降。估计模型容量，给定一个模型种类，将有两个主要因素（参数的个数 和 参数值的选择范围决定容量），线性模型参数个数 d + 1，有隐藏层的呢 (d + 1)m + (m + 1)k，这里的 +1都是指偏置。参数指选择范围大，那么模型的容量也是非常大的。统计学的理论，VC维。。对于一个分类模型，VC等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型来对它进行完美分类。2维输入的感知机，VC维=3（也就是三个点都能用直线分开类别，4个点呢，遇到XOR问题的时候就不行了，必须要一个曲线），支持N维输入的感知机的VC维是N+1，一些多层感知机的VC维O(Nlog2N)。深度学习模型的VC维很困难。loss在模型上出现先下降再上升的情况的话，基本就是过拟合了
- 通过限制参数值的选择范围来控制模型容量，通常不限制偏移b（限不限制都差不多），小的ciyata意味着更强的正则项，也就是惩罚项？？（往远点拉会，这样的话防止模型过拟合，模型复杂度降低），计算梯队的时候如果加上了正则项，那么等于w前面的变成了(1 - ny)，其中 ny通常是 < 1的，在深度学习中通常叫做权重衰减，相当于在更新权重的时候，把当前的权重进行了一次放小。权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度，正则项权重是控制模型复杂度的超参数。也就是y（兰木达），L2范数的平方。理解：限制模型在优化的时候在一个很小的范围内取参数，那你模型的空间就会变小，反之，如果参数很大的话，那么去拟合线的时候也可以取很范围很大的值，那就形成了过拟合，就是不平滑。相反，参数取值有范围了，就平滑一点，模型简单一点
- 接上面，另一个方法叫做丢弃法，dropout，效果可能比权重衰退更好。使用有噪音的数据等价于tikhonov正则，丢弃法：在层之间加入噪音（而不是在输入加噪音）。E[x'] = x，丢弃法对每个元素进行如下扰动，x' = 0 （如果概率是p），否则的话，x / (1 - p)，通常将丢弃法作用在隐藏全连接层的输出上。随机挑选隐藏的神经元丢弃了，期望不变，值变大了。推理中的丢弃法：正则项只在训练中使用：他们影响模型参数的更新，在推理过程中（预测），丢弃法直接返回输入 h = dropout(h)，这样也能保证确定性的输出，dropout是hinton搞出来的，拿了2024诺贝尔物理学奖。总结一下，dropout丢弃法将一些输出项随机置为0来控制模型复杂度，常作用在多层感知机的隐藏层输出上，丢弃概率是控制模型复杂度的超参数，参数一般选择0.5，0.9，0.1，丢弃法是每次迭代一次，随机丢弃一次的
- 数值稳定性，考虑一个d层的神经网络，计算损失l关于参数W的梯队W的梯度，其实需要d-t次矩阵乘法（向量关于向量的导数是一个矩阵），太多的矩阵乘法带来两个问题，一个是梯度爆炸，一个是梯度消失。1.5的100次方得出的数就是梯度爆炸，0.8的100次方得出的数就是梯度消失。梯度爆炸的问题，1、值超出值域（infinity），对于16位浮点数尤为严重，2、对学习率敏感，如果学习率太大->大参数值->更大的梯度，如果学习率太小，训练无进展，我们可能在训练过程不断调整学习率。梯队消失：假设用sigmoid作为激活函数，输入稍微大点的话，对角矩阵的值就比较小，梯度值变成0，对16位浮点数尤为严重，训练没有进展，不管如何选择学习率，对于底部层尤为严重，仅仅顶部层训练的较好，无法让神经网络更深（有深度问题）
- 让训练更加稳定，1、让梯度值在合理的范围里面，比如[1e-6, 1e3]里面，2、将乘法变加法，ResNet，LSTM，3、归一化，梯队归一化，梯度裁剪，4、合理的权重初始和激活函数。让每层的方差是一个常数，将每层的输出和梯度都看做随机变量，让它们的均值和方差都保持一致。权重初始化，在合理值区间里随机初始化参数，训练开始的时候更容易有数值不稳定（远离最优解的地方损失函数表面可能很复杂，最优解附近表面会比较平），使用N(0, 0.01)来初始可能对小网路没问题，但不能保证深度神经网络。一通操作其实就是要 x=f(x)，那么看激活函数，使用泰勒展开，tanh(x) 和 relu(x) 在0点附近（因为权重值大概率在这个区间）满足 x=f(x)，而sigmoid(x)并不满足，但是可以调整（scaled），比如 4 * sigmoid(x) - 2 就满足过原点了
- !nvidia-smi 可以查看是否有gpu 和 gpu的使用率，在GPU 和 CPU之间挪数据是一个非常费事的事
- 卷积是深度学习里面最重要的概念之一，使用一个还不错的相机采集图片（12M像素，1200万像素），RGB图片（三个channel）就有3600万像素，使用100大小的单隐藏层MLP，按照之前的计算公式，隐藏层的是 (d + 1)m + (m + 1)k，如果分类一个问题k为1的话，m是隐藏层个数是100，那么就是36M * 100，3.6B个元素，远多余世界上猫和狗的数量（900M狗，600M猫）。注意这里M是百万，B是代表10亿。那么3.6B参数 = 14GB存储，要非常好的GPU才能存下来。
- 两个重要原则，平移不变性，局部性。怎么从全连接层出发，利用上述两个原则，得出卷积。将输入和输出变形为矩阵（宽度、高度），将权重变成一个4-D张量，这边有个hi变成hi、j的公式的变化，一定要理解空间上的变化，输入x从向量变成了矩阵，那么对两个维度都要去进行转换，那么w也就是4维的了。V是W的重新索引，vi,j,a,b = wi,j,i+a,j+b。x的平移导致h的平移。限制一下，vi,j,a,b = va,b，就是不随 i, j的变化而变化（卷积核不会随着位置的变化而变化），一般说是二维卷积（交叉相关）。hi,j的结果，只应该是xi,j附近点的输入点的结果。不应该用远离xi,j的参数，解决方法是：当|a|, |b| > diyata的时候，就让va,b = 0 （范围约束一下）。对全连接层使用平移不变性 和 局部性就能得到卷积层。卷积是一个特殊的全连接层
- 二维交叉相关，有一个kernel在矩阵Input上扫来扫去。二维卷积层是什么呢，输入X：n_h * n_w ，核W：k_h * k_w，偏差 b 是一个实数。输出Y：(n_h - k_h + 1) * (n_w - k_w + 1) Y = X ** W + b（** 是定义的操作子），其中W 和 b是可学习的参数 输出Y会变小哦。不同的核会对应出不同的效果，比如边缘检测、锐化、高斯模糊等等
- 交叉相关和卷积这两个定义，没有太大区别，卷积在公式上有个负号，但由于是对称性，在实际使用中没有区别，因为w反正是学的。上面的例子都是二维的，其实也可以一维或者三维的，一维的话是文本、语言、时序序列。三维的话是视频、医学图像、气象地图。二维一般是主流。总结：卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出，核矩阵和偏移是可以学习的参数，核矩阵的大小是超参数（控制局部性、感受野），卷积相当于把权重减少了。。之前图片的例子，不用去训练3.6B的参数了
- 给定一个 32 * 32输入图像，应用5 * 5大小的卷积核，第1层得到的输出大小 28 * 28，第7层得到的输出大小 4 * 4（自己算一下），那么更大的卷积核可以更快的减小输出大小。到 4 * 4的时候就不能再用卷积核了，也就不能再加层了。不想要输出变的这么小的话可以用填充，即在输入的四周加上额外的行和列。填充可以由p_h 和 p_w 来表示，通常p_h = k_h -1，而 p_w = k_w - 1，当k_h为奇数：在上下两侧填充 P_h / 2，当k_h为偶数，在上侧填充P_h / 2向上取整，在下侧填充P_h / 2向下取整
- 步幅的话，给定输入224 * 224，在使用5 * 5卷积核的情况下，需要55层将输出降低到 4 * 4（每次大小会减4， 224 / 4 余个 4 * 4），需要大量的计算才能得到交小输出。步幅是指行/列的滑动步长，高度3 宽度2的步幅，就是说不是像之前往横、纵各移一格。横向 2格2格移，纵向 3格3格移，这样输出的矩阵大小也会改变。总结：填充 和 步幅都是卷积层的超参数
- 卷积的另两个超参数，通常大家也都会去设的，输入通道 和 输出通道。彩色图片可能有RGB三个通道，转换成灰度会丢失信息。输入的话有三个通道会怎么办，每个通道都有一个卷积核，就是有多少输入的维度就会有多少卷积核，最后将结果相加。无论有多少输入通道，到目前为止我们只用到单输出通道，我们可以有多个【三维卷积核】，每个核生成一个输出通道。核是4-D了，输出Y会有多个通道的（多个输入通道 -> 多个输出通道），输入 和 输出通道没有什么相关性。为什么要多个输入多个输出呢，每个输出通道在识别特定的模式（独特的特征），输入通道核识别并组合输入中的模式
- 1 * 1卷积层，它不识别空间模式，只是融合通道！！3通道的 3 * 3矩阵，要变成 2通道的 3 * 3矩阵，需要 3个输出0通道的 1 * 1卷积核 和 3个输出1通道的 1 * 1卷积核
- 计算复杂度的话，假设输入输出通道 c_i = c_o = 100， k_h = h_w = 5，m_h = m_w = 64，那么计算复杂度就是 1GFLOP，那么10层的话，1M的样本，10P FLOP（只做前向的话），CPU: 0.15 TF = 18h， GPU: 12TF = 14min。（上面是扫一次的数据，扫很多次就花费很多了），总结：输出通道数是卷积层的超参数（输入不是），每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果，每个输出通道有独立的三维卷积核（？？？？）。
- 池化层（pooling layer），二维最大池化，返回滑动窗口中的最大值 2 * 2 max pooling，（也可以有平均池化层）池化层也叫汇聚层。最大池化层让卷积输出可以向左偏移一位（最大的么，但是向右不会）。池化层与卷积层类似，都具有填充和步幅，没有可以学习的参数，在每个输入通道应用池化层以获得相应的输出通道，输出通道数 = 输入通道数，它不像卷积去融合多个通道的。多通道融合的事交给卷积去做。池化主要缓解卷积对于位置的敏感性。
- LeNet，是上面所讲的结合，在手写体上，早期成功的神经网络，先使用卷积层来学习图片空间信息，然后使用全连接层来转换到类别空间。两个卷积 + 池化再到一个多层感知机。。就是LeNet了。通过卷积，把空间信息不断压缩压缩，加到通道里面，等于高宽在不断减少，加到了通道上，通道数量变大
- AlexNet，在2012年(发表在2012年的NIPS上)，Hinton和他的学生搞出来的，引发了深度学习潮哦。。2000年的时候 SVM替换掉了神经网络，成为机器学习主流的网络，2006年凸优化的求解就很热了，现在SVM也被广泛使用，2000年左右，计算机视觉（图片啊）其实关心的是 抽取特征，描述几何，凸优化，漂亮定理，如果假设满足了，效果很好。cv中的特征工程，特征描述子：SIFT，SURF，视觉词袋聚类，最后用SVM。2010-2020年 gpu的计算能力比数据大小增长的快（量级上）。构建更深的网络，用计算去获取精度。AlexNet赢了2012年ImageNet竞赛，它本质是更深更大的LeNet。主要改进是：丢弃法，ReLu，MaxPooling（LeNet主要是Avg），同时改变了计算机视觉方法论的改变（比LeNet大了几十倍），以前特征提取（人工），放到SVM，现在CNN自己学习特征，softmax去回归。所以CNN学出来的东西很有可能就是softmax需要的东西
- AlexNet是一个更大更深的LeNet，更大的池化窗口，使用最大池化层，更大的核窗口和步长，因为图片变大了。第一个卷积，11 * 11的卷积核，96通道，步幅是4。之后第一个池化层，用 3 * 3的核（往左移一下，往右移一下都行），步幅是2，用maxPooling。第二个卷积层，5 * 5的卷积，256通道，2填充，再接一个跟上面一样的池化层，后面直接新加3个卷积层，，3 * 3，通道是384，1填充，连续放3个，最后接一个池化层。最后AlexNet也接了两个隐藏层，4096 -> 4096 -> 1000。就是更胖的，稍微深一点点的架构。另外，激活函数从sigmoid变成了ReLU（减缓梯度消失），隐藏全连接层厚加入了丢弃层（做模型的正则化），数据增强（对原始数据做随机截取，调高亮度，调低亮度，因为卷积对位置、光线啥的很敏感，那不敏感不就在数据中增加很多这种变化）（这些都是小trick）。每一层算一算要学习的参数个数，大概是46M（4600万左右），FLOP整个大概是1G（10亿次计算），比LeNet多出250倍次的计算，GPT-3有1700亿个参数。。跑起来的话比LeNet慢，4000+ examples / sec
- VGG，AlexNet思路的一个拓展，Alex不规范，如何规范起来，提出了块的概念，VGG块，3*3卷积（填充1）（n层，m通道），2*2最大池化层（步幅2），架构就是多个VGG块后接全连接层，不同次数的重复块得到不同的架构，VGG-16，VGG-19这样。VGG就是更大更深的AlexNet(重复的VGG块)，
- NiN，网络中的网络，现在用的不多，但有些思想。卷积层有较少的参数，但是卷积层的第一个全连接层的参数太多了。vgg占用了差不多700m的内存。NiN的思想就是我就完全不要全连接层。NiN块，一个卷积层后跟两个全连接层，步幅1，无填充，输出形状跟卷积层输出一样，起到全连接层的作用。架构，无全连接层，交替使用NiN块和步幅为2的最大池化层，逐步减小高宽和增大通道数，最后使用全局平均池化层得到输出，其输入通道数是类别数。参数个数特别少
- GoogleNet，GN中最重要的是 inception块，前面的方案我全要，输入跟输出等高宽，通道数变多，使用不同大小的卷积层。1，3，5的卷积，然后max pooling都有了。通道数是调出来的，跟单 3 * 3 或 5 * 5卷积层比，inception块有更少的参数个数 和 计算复杂度，因为有大量的 1 * 1 卷积，参数少，计算也会少。GoogleNet就是一大堆的inception块，分成5个stage，google的论文特别难复现，那么多超参数，，inception后续有很多变种，inception-BN(v2)，使用batch normalization，V3修改了inception块，V4使用残差连接。Google的V3还是会被经常使用的。总结，inception块用4条有不同超参数的卷积层和池化层的路来抽取不同的信息，优点是模型参数小，计算复杂度低，GoogleNet使用了9个Inception块，是第一个达到上百层的网络！！后续有一系列的改进
- 批量归一化层（2016年左右，就是BN），效果很好，要做很深的神经网络的话，是一个不可避免的层。深的话，损失出现在最后（损失函数哦），后面的层训练的较快，forward的时候从数据到损失，反向传播的时候从损失到数据，可能一开始梯度还算可以，到后面就越来越小了（到数据那侧）。也就是靠近损失的地方会收敛的很快。数据侧会比较慢，但是其实下面的数据一遍，上面也得跟着变，最后那些层需要重新学习很多次，导致整体收敛很慢。所以能不能在学习底部层的时候避免变化顶部层么？？就是批量归一化，固定小批量里面的均值和方差，然后再做额外的调整（可学习的参数），可学习的参数是伽马和贝塔，作用在，1、全连接层和卷积层输出上，激活函数前 2、全连接层和卷积输入上。批量变化是一个线性变换。全连接层作用在每个特征上，卷积层作用在通道层上面（这里一定要分清楚），最初的论文是想用它来较少内部协变量转移，后续有论文指出它可能就是通过在每个小批量里加入噪音来控制模型复杂度，因此没必要跟丢弃法混合使用。这相当于是工程走在了前面。批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放，可以加速收敛速度，但一般不改变模型精度。这样就可以调整更大的学习率，可以从0.01调整到0.1，以前0.1的话，上面收率很快，下面炸了，只是为了加速收敛，不改变模型的精度的
- 假设在卷积神经网络中要了解一个网络的话，就去看ResNet吧，模型学出来的F要有个包含关系，这样使得模型不会越学越差，有可能模型就越学越歪了（loss离最优点越来越远了）。这里就用到了残差块，串联一个层改变函数类，我们希望能扩大函数类，残差块加入快速通道来得到，f(x) = x + g(x)的结构。这个块可以有不同的形式，ResNet块，高宽减半ResNet块（步幅2），后接多个高宽不变的ResNet块。带+号的整体叫作ResNet块。ResNet架构，类似VGG 和 GoogleNet的总体架构，但替换成了ResNet块。ResNet-152（152个卷积层）。总结，残差块可以训练很深的网络，甚至1000层都行，残差网络对随后的深层神经网络设计产生了深远的影响，无论是卷积类网络还是全连接类网络
- GPU、TPU，数字信号处理芯片（DSP），可编程阵列（FPGA），AI ASIC（AI 芯片），Google的TPU是标志性芯片，性能能媲美nvidia gpu。ASIC就是比较定制呀，大厂愿意去这样做，领域不好就没有用了，硬件的研发周期，投入都很大
- 一台机器可以安装多个GPU（1-16），在训练和预测的时候，一个小批量计算切分到多个GPU，常用的切分方案有数据并行（通常性能更好），模型并行（通常用于模型大到单GPU放不下），通道并行（数据 + 模型）。多gpu就是梯度在一个gpu上加起来，再复制回其他的gpu，框架中的主要的 pytorch中的 dataparallel，用dataparallel，数据集也会被自动分配到多个GPU上
- 分布式训练，分布式训练中，考虑带宽，考虑大数据集，考虑高效的数据读取和预处理，有好的计算（FLOP）通讯（model size）比，就是计算时间要高于数据传输时间 10% - 20%，一般（Inception > ResNet > AlexNet)，batch_size大一点吧
- 数据增广，数据增强，在语言里面加入各种不同的背景噪音，改变图片的颜色和形状。比如左右翻转，上下翻转，切割，改变色调、饱和度、明亮度。数据增强通过变形数据来获取多样性，从而使得模型泛化性更好，常见的就是翻转、变色、切割。
- 微调在计算机视觉或者深度学习中非常重要的东西！所谓的微调，也叫作transfer-learning，迁移学习。还有一个Fine-tuning呢？？？（就是FT呗，微调就是一种迁移学习），在我的目标数据集上，可能仍然可以对我数据集做特征提取，但又不能直接使用，可能是标号变了。在原数据集上有个pre-train的模型，在我的训练集上用一样的模型（你用ResNet18，我也用ResNet18，但是我模型参数的初始化用你的，除了最后一层，输出层），这样反正train的也会比较快么，因为loss是从上面下来的。这样使用更强的正则化，使用更小的学习率，使用更少的数据迭代。源数据集远复杂于目标数据，通常微调效果更好。还有重用分类器权重，源数据集可能也有目标数据中的部分标号，比如车啊，飞机啊，可以使用预训练好模型分类器对应标号对应的向量来做初始化。还有个是固定一些层，神经网络通常学习有层次的特征表示，低层次的特征更加通用，高层次的特征则更跟数据集相关，可以固定底部一些层的参数，不参与更新（更强的正则）
- 图片分类 和 目标检测，目标检测会更复杂一点，因为还要确定位置，bounding box，边缘框，一般就是左上、右下确定框了，或者 左上的点后知道宽和高。标注就是找人在图片上画框。。标注的成本太高了，数据集一般比较小。目标检测数据集，COCO数据集。80物体，330K图片，1.5M物体。COCO在目标识别中跟imageNet算一个量级的数据集了。
- 锚框，预测每个锚框里是否有关注的物体，如果是，预测从这个锚框到真实边缘框的偏移。IOU-交并比，A交B / A并B，0表示无重叠，1表示重合，jacquard的指数。给锚框标号，每个锚框是一个训练样本，要么标注成背景，要么关联上一个真实边缘框，可能会生成大量的锚框，这个导致大量的负样本。使用非极大抑制（NMS）输出。一类的目标检测算法都是基于锚框来预测的
- 物体检测算法，R-CNN，使用启发式搜索算法来选择锚框，使用预训练模型来对每个锚框抽取特征，训练一个SVM来对类别分类，训练一个线性回归模型来预测边缘框偏移。fast-rcnn（不再对每一个锚框坐特征提取，对一张图片做，使用ROI池化层对每个锚框生成固定长度特征）。Faster R-CNN（）。Mask R-CNN（）。
- 转置卷积，卷积不会增大输入的高宽，通常要么不变，要么减半。转置卷积则可以用来增大输入高宽。操作子就是跟卷积反过来。转置卷积为什么需要把图片再变大，因为需要每个pixel的信息。要得到每个像素的标号的话，通常是用转置卷积（语义分割是对每一个像素做label的预测，不是去进行还原）。语义分割能不能做主要还是看cnn的网络能不能淦
- 全连接卷积神经网络（FCN），FCN是用深度神经网络来做语义分割的奠基性工作，它的转置卷积层来替换CNN最后的全连接层，从而实现每个像素的预测
- 迁移学习，其中的style怎么匹配，匹配两者的RGB值的 一阶二阶三阶信息，一阶统计信息就是均值，二阶就是协方差，tv降噪
- nlp，时序序列是T个不独立的随机变量，预测的东西跟图片分类不同，图片的输入输出不是一个东西，而nlp的预测和输入都是一个东西，叫自回归模型。两种实现方式，马尔科夫模型假设当前只跟最近少数数据相关，从而简化模型。潜变量模型使用潜变量来概括历史信息。rnn是个潜变量模型。
- nlp的样本就是tau特征数目的样本，label就是下一个数据。NLP里面中文分词是很大的一块
- 语言模型，bert，gpt都是语言模型。生成文本，给定前面几个词，不断的使用p去生成后续文本。判断多个序列中哪个更常见。二元语法、三元语法用的很多（会有序列属性）。语言模型是估计文本序列的联合概率，使用统计方法时常采用n元语法
- 序列模型的神经网络开始，第一个，RNN。先要看下潜变量自回归模型，使用潜变量h_t去总结过去的信息。~~生成o_t的时候不能看到x_t。输出发生在观察之前，所以计算损失的时候是o_t和x_t之间的比较，而x_t也可以用来更新h_t，让它挪到下一个单元。RNN本质上是一个MLP，但是带了时间轴在隐变量层，把前面的信息引入了（用W_hh去存储）。~~ 不过这边slice好像弄错了，看书的话应该是h_t 和 h_t-1有关，但是和x_t也有关，输出层没有变，用当前的h_t。另外ht到ot还需要一个线性变换，这个跟MLP一样
- 困惑度（perplexity）。衡量一个语言模型的好坏可以用平均交叉熵。语言模型其实也是个分类模型。。就是下一个词是哪一类。做n次预测那就是交叉熵的n次平均。但历史原因NLP使用的是困惑度exp(pai)来衡量，就是再做个指数，叫做困惑度。会使值变大。1表示完美，无穷大是最差情况。
- 梯度裁剪，防止梯度爆炸的，太大了，NaN，将梯度拉回到seta。假设用T段去预测，那么潜变量层的数量基本就是T了，也就是深度神经网络的层数，为了防止数值稳定性问题，要适当进行梯度裁剪。应该是这么说，就算一个隐藏层的rnn，那么在隐藏层单元也要做 T （至少）次矩阵乘法，相当于一个 T 层的量。但是num_step不能等同于隐藏层的层数
- GRU门控循环单元，想只记住相关的观察需要，能关注的机制（更新门），能遗忘的机制（重置门/遗忘门），R 和 Z，还有候选隐层状态。因为激活函数sigmoid是0-1范围内的，靠近0的部分元素乘法后，相当于之前的记忆就消失了。R_t是可以学的。R决定是否用H_t-1。Z_t干嘛的。。决定一些比例么（决定用的当前的X_t的信息要用多少）。Z为0，R为1的情况下，就退化成正常的RNN。另一种极端情况就是直接忽略当前的X_t(这部分主要看公式)。
- LSTM，长短期记忆网络（90年代发明的？？），忘记门，将值朝0减少，输入门，决定是不是忽略掉输入数据，输出门，决定是不是使用隐状态。gru是根据lstm改进的。候选记忆单元。LSTM的话。。算了把，看着太复杂了
- 深度循环神经网络，怎么把深度变深呢。除了第一个隐藏层H用到了输入，其他的隐藏层都是上一个H。深度循环神经网络使用多个隐藏层来获得更多的非线性性。pytorch的框架是不带输出层的。深层RNN，每层都需要一个初始的hidden state
- 双向循环神经网络，rnn只是看过去，而双向是一个前向RNN隐层，一个方向RNN隐层，合并两个隐状态得到输出。合并是concat起来。但是这个东西做推理不好推。推理给你两个token，你去推下一个，双向是你能看到未来。双向的作用是对一个句子提取特征，语音也可以等你把话说完。比如做翻译的时候是可以开双向的，双向也能深度的
- seq2seq，这个最早就是做机器翻译了。目前听说google的搜索和翻译都用了bert。seq2seq就是一个encoder 和 decoder的架构。它的编码器是一个rnn，读取输入句子，可以是双向的。解码器使用另一个rnn来输出。双向可以做encode，但是它不能做decode。一个变长的东西 到 另一个变长的东西的翻译。编码器是不需要输出的，拿到最后的RNN的状态就行了。注意，训练的时候，解码器使用目标句子作为输入的。衡量生成序列的好坏的bleu。
- 束搜素，贪心和穷举之间的平衡，贪心是最快的，穷举是最好的。束搜索，保存最好的k个候选，在每个时刻，对每个候选新加一项（n种可能），在kn个选项中选出最好的k个。beam search。
- 注意力机制，来了，不随意线索 和 随意线索（随着意志。。），之前卷积、全连接、池化层等只考虑不随意线索（就是第一眼的东西，大的亮的pixel等等）。注意力机制则显示（是显示的加入啊！！！）的考虑随意线索，被称之为query（query就是我自己有个想法去干嘛），每个输入是一个值（value）和不随意线索（key）的对，通过注意力池化层来有偏向性的选择某些输入，attention pooling。从非参数化的注意力机制到参数化的注意力机制，加入一个可以学习的w。x是key，y是value。阿尔法是权重，早在60年代就有非参数化的注意力机制了，核函数K越小的话距离越近，越大的话距离越远（或者反过来，越大距离越近，越小距离越远），不需要学习任何参数，有点像KNN，K核使用高斯核，把式子带进去，权重部分等于坐了一次softmax，在之前的基础上引入可以学习的w，包在整个softmax里面，w是一个标量，不是一个向量。权重的那部分就叫作注意力权重，主要也是计算这个权重
- 注意力分数，权重一般是>=0，加起来等于1，注意力分数就是attention scoring function（注意力评分函数，没有被normalized），keys（x）和query跟asf作用，经过softmax得到attention weights，合并values（y）得到输出。看书，上面的例子query是一个值，在这边query就是一个向量。所以就要看asf这个函数a是如何设计的，第一种叫做additive attention，可加性注意力，可学参数有3个，等价于将key和query合并起来后放入到一个隐藏大小为h，输出大小为1的单隐藏层MLP（确实！！），好处是key啊，query啊，value啊这三个东西可以长的不一样。另一种，如果key和query都是同样长度的，那么可以不学东西了，向量q，k作内积，然后除以根号d。scaled dot-product attention，其有对应的向量化版本，Q是n个query，K是m个key（拓展到多维）。总结：注意力分数就是query和key的相似度，注意力权重是分数的softmax结果
- 注意力的实现，编码器对每次词的输出作为key和value（key和value是一个东西，英语句子长为3的话，就会有3个key-value的pair），作为key-value放进attention里面，解码器RNN对上一个词的输出是query，注意力的输出和下一个词的词嵌入合并进入RNN。seq2seq中通过隐状态在编码器和解码器中传递信息，注意力机制可以根据解码器RNN的输出匹配到合适的编码器RNN的输出来更有效的传递信息
- 自注意力池化层将向量x当做key、value、query来对序列抽取特征得到y_1，y_2。。。也就是 key，value，query都是自己。cnn、rnn、自注意力都可以用来处理序列。自注意力，transformer，gpt模型的计算量都非常非常大，用几千个gpu算。跟CNN/RNN不同，自注意力并没有记录位置信息，位置编码将位置信息注入到输入里，使得自注意力能够记忆位置信息
- transformer 架构，基于编码器-解码器架构来处理序列对，跟使用注意力的seq2seq不同，transformer是纯基于注意力的（完全self-attention，里面没有rnn了）。也有位置编码，有transformer块。多头注意力机制，对同一key、value、query，希望抽取不同的信息，例如短距离关系和长距离关系，多头注意力使用h个独立的注意力池化，合并各个头（head）输出得到最终输出。解码器对序列中的一个元素输出时，不应该考虑该元素之后的元素，可以通过掩码来实现。还有基于位置的前馈网络（FFN，就是全连接，或者1*1的卷积也行），还有层归一化LN（不同于BN），编码器中的输出y_1，y_2。。y_n，将其作为解码中第i个transformer块中多头注意力key和value，它的query来自目标序列，意味着编码器和解码器中块的个数和输出维度都是一样的。有两个是自注意力，但是传递信息的那个是个正常的注意力，。。编码器 和 解码器都是用的n个这样的transformer块
- NLP里面的迁移学习，使用预训练好的模型来抽取词、句子的特征，例如word2vec或语言模型，17年18年，基于深度学习的nlp模型出来了，解决nlp里面的迁移学习。bert是只有编码器的transformer，两个版本base/large。1个亿的参数/3个亿的参数。bert用了整个Wikipedia训练。大于3B词（30亿个词），很深的网络，Bert是LLM的始祖

- Bert微调，bert是没有label的，用钱去训练大量数据
- 优化算法，一般来说，迭代优化算法只能保证找到局部最小值。梯度变成0后，不管学习率是多少，都不会动了。凸优化，任意两个点，连一个线，还在这个集合里面。凸函数呢，函数上找两个点，连线，保证函数都在连线的下面！！！如果函数是一个凸的，且限制集C也是一个凸的，那么就是凸优化问题。那么局部最小一定就是全局最小。严格凸优化问题有【唯一】的全局最小。目前为止就两个是凸的，一个是线性回归，一个是softmax回归。其他都是非凸的，MLP，CNN，RNN，attention等等。梯度下降要遍历所有样本算一次梯度，太贵了，随机梯度下降随机选个样本算，两者的期望相近。真正用的是小批量随机梯度下降。计算单样本的梯度很难完全利用硬件资源，这是一个无偏的近似，但降低了方差。批量很小的时候，收率会快但是计算慢，批量很大的时候，计算量也大（性能会好一点）。。但是收敛会慢。冲量法，（momentum），使用平滑过的梯度对权重更新，维持一个惯性，看之前时刻的一些g。还有个方法，Adam，不一定会比sgd + momentum好，但是它做了非常多的平滑。对学习率不敏感。Adam对梯度做平滑，且对梯度各个维度值做重新调整。现在adam会用的比较多

- gpt的论文开始，generative pre-training，gpt（论文的名字），gpt是半监督的方法，非标的数据集上pre-train，在fine-tuning到有标的小的数据集上。但是后面半监督学习的名字换了，叫作自监督学习（Clip的算法），unsupervised pre-training，看论文，它只拿了transformer的解码器，因为它unsupervised pre-training的时候用的是前k个词的似然，不能往后看，对应trans的解码器中的mask attention，然后第二个attention去掉，因为没有encoder的输入作为k,v了，是cross attention。而bert是完型填空，把中间词挖掉，预测挖掉词，所以它能看见前后，可以使用编码器。gpt用了一个更难的目标函数（损失函数），因为预测未来肯定比完型填空难。supervised fine-tuning，也有个目标函数，两个目标函数相加。训练的时候，无论输入/输出怎么变，但是基于transformer的模型结构是不变的。

- gpt2用了多任务，就是在多个数据集上进行训练，zero-shot，不需要下游任务的任何标注信息，训练一个模型，在任何地方都能用，因为下游任务不用标注，不能构造那些模型不认识的符号，gpt-1中的那些start、extract等等，输入的形式应该更像自然的语言，跟预训练阶段一样。假设要做翻译，没有特殊符号，就换成 translate to french（这个就是prompt，提示词！！！！！，2018年出的一个论文），English text，french text，那在做阅读理解的时候就可以这样写（answer the question，document，question，answer）

- gpt3尝试去解决gpt2的有效性，把zero换成 few-shot了，1750亿个可以学习的参数。。。(对比非稀疏的权重矩阵)，就算给一点（few-shot）的样本，gtp-3不做梯度更新（因为太大了）的或者微调的，解决方法in-context learning。每个子任务提供10-100个样本，one-shot learning，和 zero-shot learning。批量大小，320w。。。（超级大批量），用了LSH，需要分布式训练。所以gpt-3就在few-shot的时候已经没有可以学习的东西了？？？？？

- 2021年的codex，基于gpt的模型，相当于一个应用。微调了github上的code，gpt-3并没有训练代码。模型结构本质没有区别，数据集变了，权重变了，这套就叫做codex。代码上用bleu分数不好用，它不是近似，如果只是用pre-training的lm的话，converge(收敛)更快点，在实验部分出现了温度（temperature）,在算softmax之前，给线性层的输出除以一个温度T，默认是1，T比较大的话，softmax出来的值会比较一样，概率比较相近，低的温度T，会拉的比价开。因为是基于采样，这些值会最后影响我们的采样。因为根据概率采样，低温度让你总能取到几个那么好的值，如果比较高的T，不是最好，但是一般般好的值被采样的概率也会增加了。还有个二次微调。。带监督，认为数据集市有标准答案的

- GAN，开干。generative adversarial(对抗的) nets，机器学习两大类，分辨模型（分类，预测数值，discriminative），生成模型（怎么样生成这个数据的本身），最后有yoshua bengio。一个生成模型G就是对数据建模，还有个辨别模型D，来估计一个样本到底是从训练数据来的呢 还是从模型G来的呢。生成模型的任务是尽量的让你的辨别模型犯错。G 和 D是一个MLP的话，整个系统就可以通过一个反向传播来进行训练，这里不需要使用markov链，也不要近似的推理展开。以前是真的学一个分布，有参数param，把均值啊，方差啊全部学出来（最大似然likehood）。现在是就学个模型来近似你要的结果就行了（end to end），坏处是最终并不知道分布长什么样子。对f的求导 等价于 对f的期望求导。

- vit，也是比较划时代（2021年），等于nlp助攻了cv，把transformer那套打法扔给了vision。vision transformer。。第一个将transformer应用到计算机视觉任务的。google发表的。直接抛弃CNN了。处理的问题，如何把2D的图片变成1D的序列，怎么把自注意力用到视觉里面来呢。把一个图片分成很多的patch，特么cv和nlp两个领域大一统了。后面的方向是detection 和 分割啊，如何像nlp一样进行自训练了。看网路的话，它就像bert一样只有encoder，重要的是如何转换图片到token，分类借鉴了bert里面的cls token。自注意力是否能运用到vision，在实验部分是有的

- CLIP(contrastive language-image pre-training)，openai的(contrastive 是对比学习吧)，把文本当做一个训练的信号，学到的不单单是一个视觉特征，而是多模态特征了。2021年，做了很多实验，包括ocr，视频动作识别，learning transferable visual models from natural language supervision。牵扯了文字、图片的多模态的工作。对比学习的方式去训练。完全没有手工标注，收集数据集，4亿个图片和文本的配对（自己做的，WebImageText数据集）。做zero-shot的推理。没有分类头的（softmax哈，卧槽，有点牛逼了，不分类，来个新的三轮车，没训练咋办，是不是数据量太大了？？ ），用prompt + template。把图片用句子描述，任意给一张照片，通过给模型去喂不同的文本句子，从来知道这张图片里面有没有感兴趣的物体。自己创建了数据集，然后这个数据集也影响到了图像生成的工作（DALL-E），之前openai都是喜欢gpt化，但是到clip的时候它们选择了对比学习（这个图片 和 这个caption是不是对的上、判断题），预测型的目标函数换层对比型的目标函数，算余弦相似度，模型实在太大了，费钱，temperature对实验影响比较大，作为一个标量进行学习了。还有不少trick，混精度训练？？工程性质的优化，实验部分zero-shot的推理。输入图片到image encoder，词的话按类别通过prompt engineering变成句子输入到text encoder，跟图片的做余弦相似度，最后还会接一个softmax。prompt engineering（还有ensembling）在cv和nlp都是很火的，咒语工程师，为什么要prompt，因为单词有多意的，不同语境下对应的意思不同。文本encoder的时候通常都是句子，如果进一个单词就会有distribution gap。【a photo of a {label}】，prompt template，首先这肯定是一个句子了，然后{label}肯定是一个名字（减少歧义性）。如果你知道数据集是动物，提示的时候还可以再加一句话【a photo of a {label}, a type of pet】。比如ocr里面给想要找的内容打上 双引号 ""，也算是一种提示。ensembling就是把提示模版整合起来，论文中作者说了用了80个。最终实验部分证明zero-shot 迁移是可以被应用的，对比resnet50等等。这边还提到了linear probe（就是把之前训练好的冻住，再训练一个分类头就行了），而fine-tuning之前网络的权重也要更新的（网络都放开，做end-to-end的学习）。微调更灵活的，linear probe不太需要调参。用去重的方法去验证是不是数据集太大了，才导致的泛化性能好。image caption baseline。有点把人工智障变成人工智能的趋势，clip的代码在github。最后还有代码演示。。

- DALL-E 2

- CoT，chain of thought，2022年google提出的论文Chain-of-Thought Prompting Elicits Reasoning in Large Language Models，语言模型的规模衡量，1是训练计算量floatps，2是训练数据的大小num of tokens，3是模型本身参数量的大小。就是大语言模型在遇到推理任务时候的局限性所出的。system 1 任务，system 2任务就需要很慢很仔细的考虑。prompt给出人为写的中间推理步骤，没有prompt的就是大语言模型的下限

- 多模态，比较好的loss函数可能就是ITC（clip）、ITM（二分类？非常像正样本的负样本（hard negatives））、MLM（Bert里面那个Mask）。ALBEF（salesforce research，align before fusing），multimodal encoder里面有 cross attention。多模态要体现一个多模态融合的过程。为了做momentum distillation，还有个momentum model，ALBEF里面的动量蒸馏。BLIP（也是salesforce的人做的）结合了ALBEF 和 VLMO等论文，有ITC，有ITM，最后采用文本生成的（gpt的思路，就是LM，这区别于之前提到过的MLM）去做loss，也是三个loss，BLIP有image caption了，能改变网页爬取的内容，转换成直接描述的图片（caption filter）。CoCa（constrastive Captioners are imageNet-text foundation models），google的一篇论文，看名字差不多知道，constrastive loss（类似itc咯） + captioning loss（给多模态的loss，就相当于blip中的LM的loss），六边型画图法。BeiTv3（微软的），用的差不多都是public的数据集，这篇的引言非常好，总结了之前的多模态相关工作。

- gpt4，说是可以多模态了。只是输入端可以接收图片，2023年3.14号发布很多，包括microsoft的copilot。同周，pytorch出来了2.0版本，openai一开始是马斯克相关，后面给微软投了，gpt-4的时候通过azure重构了deep learning的部分。RLHF的方法去微调模型（instruct GPT），RLHF是对模型做控制，知道我们想做什么，想问什么。GTP-4对数学上不是很行，还有编程和英语。GPT4对多语言支持是可以的。除了prompt外，加了system message，（口气），system：你现在是一个科学家。家庭教师

- instructGPT，2022年3月，chatgpt，用了和instructGPT一样的方法，是一个prompt的模式。它是从gpt-3.5上过来的。语言模型跟人类的意图做aligin，还是在标注，reward model（rm模型），最终PPO（强化学习的模型）。pair-wise的ranking loss（用了一个 逻辑回归），强化学习用的算法是PPO，强化学习的模型叫作 RL policy，critic网络，和一个K-L散度。这边的 RL相关的式子可能是需要一些基础的（https://arxiv.org/abs/2203.02155）

- llama3，meta的开源的论文，2024年的时候，这里面已经出现了deepseek，code and reasoning data会对 代码，推理比较好，后面做定制话的rag啥的比较好。数据全是苦力活。grouped query attention，不用的话解码的时候系统压力比较大。k-v cahce，特别占用内存，H100 GPU。讲了不少计算、存储、网络的东西的，怎么建的机房的时候，上层交换机、汇聚交换机等等。跟网络相关的有进行数据的切割，平均2小时训坏一次，做好checkpoint，换机器，重启任务，一般都是gpu、网络的问题引起的。还有debug的事情。。。1w张卡上做训练，训练的时候序列就是8k toekn，所以很多模型的上下文支持长度就是8k。长文本训练，相当于调参了，从8k一步步到128k。最后的退火。没有讲数据采样部分。。。

- sora，视频clip到 4-16s，然后会经过很多的filter，motion filter，content filter等等，

- bert，还是要看下bert的论文，bert_large的模型是3.4亿的参数，而gpt-2直接干到了15亿，训练集是百万级别

- 考古下 toolformer，metaai出的，就是可以去调各种api的

- moe架构

- rag，retrieval-augmented generation，检索增强生成

- 考古下 Word2vec，

- 考古下 clip 和 vit

- 考古下 判别式网络 和 生成式网络，mae论文，判别式和生成式模型举例的话 svm 和 贝叶斯？？

- 对抗性样本

- dense的模型 和 moe的模型

- 去重的方法 和 数据清理

- 数据集也得看minhash，de-duplication（url-level de-duplication, document-level de-duplication, line-level de-duplication），n-gram，

#### 一些背景

1、深度学习差不多在2014年前，把一些之前的东西，或者别的东西拿出来，改个名字，炒冷饭，后面就有新的东西了

2、用MLP，比用个SVM还好呢，为什么呢，因为MLP效果不好的话还可以用个卷积啥的，用个transform啥的，因为对神经网络来说，模型虽然变了，但大体结构是一致的，可调整

3、神经网络讲的一层通常是包含激活函数的，通常的一层也是带权重的一层，输入层级不算层了，每一个箭头就是w么，那么数层的时候看箭头也行

4、MLP应该是在SVM之前的，MLP没有流行就是一个你得选超参数，要调多少个隐藏层啊，调单元数啊，收敛也不好收敛，SVM（基于kernel的）对于超参数不敏感，优化不需要用SGD，两个模型在实际效果上差不多的话（可能MLP效果都要好一点），但还是会用SVM，因为数学好一点（非常漂亮的数学定理），所以SVM从90年代到00一直是机器学习的主流。

5、一层感知机，理论上可以拟合任意函数。。但实际上做不到，因为优化算法解不了

6、为什么神经网络要增加隐藏层的层数，而不是神经元的个数。只能说模型的复杂度是差不多的，但是胖的不好训练，深一点的就叫深度学习，浅一点的就叫浅度学习，胖的特别容易过拟合，深的话就训练的更好

7、激活函数的话就用relu吧，sigmoid，tanh啥的其实也没啥好选的（本质上差距不大），不如调整神经元个数或者层数

8、对于一个未知的数据，假设输入128，输出是个2，其实可以先试下线性的，然后增加一个隐藏层的（比如神经元是16），再试32，再试64，再试128都行，比如16、128都不行的话，32，64还可以，那么会加入第二个隐藏层，32后面 加个8

9、svm理论上分类还是不错的，缺点在哪？数据量上，SVM很难做到100w个点，很难算，能调的东西确实不多。看看神经网络的优点呢，神经网络本身是一种语言，通过构建网络来表达你对世界的了解。SVM的可编程性差点，能比神经网路解决的问题少，仅从分类角度上讲，svm问题不大。svm做image-net就不行。神经网络可以通过卷积做比较好的特征的提取。SVM本质是个分类器，前面还有特征的提取，而神经网络把特征提取和分类能放到一起做

10、如何设计超参数，是不是需要搜索，搜索用贝叶斯还是网格、随机，推荐的话是靠专家经验，推荐随机，就是选一些参数的组合，然后试着去炼丹，然后选一个最好的，hpo的领域

11、神经网络说是一个科学，但其实是门工程，然后可能50%是艺术。蒸馏

12、权重衰退在控制模型复杂的方法中效果不是那么好，还是有别的一些方法的，weight_decay 或者叫做 lambd，一般就是 1、-1、-3、-4

13、dropout丢弃了一部分的隐藏神经元，那么丢弃的那部分的w是不会被更新的，hinton的意思就是随机采样一个小网络去更新w

14、深度学习没有正确性可言，可能输出层丢掉一层转换后，精度也就只掉0.多或1个百分点，dropout把随机种子（rand_seed）固定住，应该是差不多的，可以做到可重复，如果用CuDNN，要禁掉，因为CuDNN出来的随机性挺大的，浮点数的精度问题，相加的顺序不一样，得到的结果不一样。可重复不是必须的，随机性能让模型变得更平滑

15、BN是给卷积层用的，dropout是给全连接层用的，两者没有太多相关性，然后慢慢就发现dropout可以不仅仅是参数啊，可以是输入，可以是标签。。后面就会出很多各种各样的dropout了。就是怎么样把一个东西搞成0。比如dropout=0.5，推理时输出结果是训练的翻倍么？不会的，因为数学期望不变的

16、权重衰退使用的地方可能还更多，droupout只能用于全连接层，权重衰退那个lambd不是很好调，dropout这个0.1，0.5，0.9还是很好设置的，假设我一个模型单隐藏层64神经元，感觉训练下来没什么问题（不会过拟合），那么我们就能换成128神经元然后开dropout等于0.5，就相当于一半被丢弃掉了呗，也就是dropout调起来比较方便。dropout会影响lr么？？还是期望的问题，不太会影响，但是会影响收敛速度，速度会变慢

17、遇到训练的好好的，测试集精度有问题的，一开始涨后面平稳的，大概率是练废了（权重学习到的是一对废参数），那么可能就是上述数值稳定性的问题

18、正态分布、均匀分布确实比较容易算，但不是强求，只要你的数据符合均值为0，方差固定的分布就行。假设独立同分布也是为了简单起见，用于初始化权重

19、实战可以用来去练调参

20、colab可以去用些免费的gpu，gpu的能力主要在于显存、计算能力（每秒能计算的浮点数）、还有就是价格了。云上的Server级GPU，和自己买的消费级GPU，GPU是通过不断的加核的方式来突破摩尔定律的。gpu满负荷就满负荷，但是温度不要超过80度，一直这么高的话，水冷风扇啥的都要用上

21、项目的显存不够用怎么办，如果batch_size调小，显存够用了，但是cuda占用一直很低怎么办呢，（可以把模型，就是网络调小）

22、初始化关于data的可以在cpu上做，但是计算要跑到gpu上（去做前向和反向运算），GPU上的推理就是inference（就是只做forward），训练就是算梯度

23、是不是要在苹果的M系列芯片上做DL呢，可以尝试，M是不错的芯片，但是你买不到额外的M的芯片

24、house竞赛的话可以使用 autogluon、然后是h2o、然后是随机森林，都使用了集成学习，随机森林就是用了集成学习思想，集成学习会让模型变得稳定。automl？？（搞了半天跟automl比无法用）。数据科学家80%的时间处理数据，20%的时间调模型。automl能节省10-20%时间。如果纯纯数据不做清理的话，给automl可能没啥用，automl就是一种简化。数据非常重要

25、用随机梯度下降方法优化人生，压缩即智能

26、图片本身就是一个二维的矩阵。只是在用softmax做回归的时候，将输入reshape成了一个向量。做一个卷积层很大的，但是层数少的，不如每层很小的，但是做深一点，跟MLP是一个道理。3*3 和 5*5的核是比较常见的

27、卷积是有完整的数学含义的，数字信号处理里面的

28、CNN的几个超参数影响重要程度是怎么得到的，核大小，填充，步幅。步幅不为1觉得是计算量太大才去调整，一般取2，填充一般就是核-1，实际中不会过多调节。已经有很多经典网络了，对应不用结果，自己设计卷积核的情况多么，（一般其实套用经典网络结构够了）

29、有什么方法能让超参数也跟着学习么？ 有的 NAS，自动机器学习部分

30、3 * 3 的卷积核 3层  是不是 和 5 * 5的两层差不多，应该是效果差不多的，但是5层更贵，计算跟核的 大小平方有关

31、通道相关，每一个通道的卷积核是不一样的（都说的是大小，其实就是多输出的时候那个c_o维度上的东西，这个时候卷积核的大小可以不一致），不同的通道的卷积核大小是一样的，这个可能需要理解一下了。二维卷积核 和 三维卷积核 仍旧是 矩阵 和 立体的区别，就是还会增加 c_o 和 c_i （3d卷积在视频中用的比较多，一般2d卷积）

32、池化核大小 和 步幅在一些框架中是一致的，说明移动之后是没有重叠的

33、stack是升维拼接，cat是等维拼接

34、一般池化层 放到 卷积层后面，池化层目前用的也比较少了（卷积 + stride就能减少输出，数据本身会做增强、移点啥的，卷积本身能看到数据发生很大的变化，不会过拟合到某个位置）。可能最后会有用一个池化层，汇聚到单数字。图片本身某列有个1，那么偏移了一下，池化会帮你在左边也添加一些1，就是降低对位置的敏感度了

35、LeNet比MLP简单啊，一个受限的MLP，模型复杂度比MLP小，那么overfitting的概率就小啊

36、LeNet高宽减半的同时把通道数翻倍

37、当你的输入数据不大的时候 几百几千维的时候，可以用MLP试试，当输入数据维度很高的时候，比如 200 * 200的图片，用CNN。通道 6 -> 16 改成 5 -> 15，效果差不多，可以练下丹

38、CNN explainer 可以看卷积到底学到的是什么，一个小哥做的网站

39、CNN 的激活函数不用sigmoid，用relu的话，不收敛，那么调下。。lr试试，把lr从0.9调到了0.1，特么收敛了，但是有overfitting

40、现在的深度学习可能不需要较大的数据集，因为有很多产业已经有很多数据，训练好的模型，再fine-tune到小数据集上

41、image-net还算是比较流行的，nlp领域的一些transformer、bert、deepfm跟cv领域的cnn区别，其实本质区别不大，万变不离其宗

42、AlexNet会提及Local Response Normalization，lrn，4096 * 4096的全连接层是一个非常厉害的模型

43、torchscript 可以转成 c++语言 部署到线上

44、softmax在层上是不体现的，在交叉熵中已经做了!!!!!，就是在training的时候，torch的文档，交叉熵中包含softmax。

45、1 * 1的卷积层其实是去降通道数的，经典网络的话 一般不要去调结构，通道数可以看着同时减半啥的

46、深度学习中的小trick很重要

47、batch_norm要出现在很深的网络中，浅层MLP和BN搭配效果不一定好的

48、epoch数、batch_size、学习率是相互 相关的，根据你的内存情况调batch_size，epoch观察一下，收敛的话就停掉。调batch_size，看你的处理样本数能否维持到某个数目

49、残差是个什么概念呢，来个小模型x，然后再贴贴补丁，类似信号的分解

50、ResNet为什么能训练1000层，防止梯度消失的方法就是乘法变加法，加法保证原有该有的梯度还存在，就算g(x)乘上了小梯度，相当于走旁路去到了data底层

51、深度网络，底层比较难训练，因为底层拿到的梯度已经比较小了，是个累乘

52、对于计算密集型的代码，超线程（一个物理核变成两个逻辑核）没有什么用，因为两个逻辑核共享一个寄存器，一个逻辑核在用着寄存器，另一个逻辑核也得等着，gpu的核（大核中的小绿点）要远远的多余cpu。cpu一般/高端 差不多 6 / 64，gpu一般/高端 差不多 2K / 4K，TFLOPS对应就是0.2 / 1 和 10 / 100。内存带宽，cpu是30 GB/s / 100 GB/s，而gpu是400 GB/s / 1TB / s。内存里面读取数据到 cpu/gpu。gpu通过核数和带宽让计算很快，内存反而只能掐到 16GB / 32GB。cpu是一个通用计算单元（还要if/else啥的呢），而gpu的控制相对就弱了

53、如何提升gpu的利用率呢，其实跟cpu是一样的，并行，使用数千个线程，至少得1000维把，内存本地性，缓存更小，架构更加简单，少用控制语句（支持有限，同步开销很大），不要频繁的在cpu和gpu之间搬数据，越小越精确，epoch数少，大的话，要慢慢揉碎掉才好

54、分布式主要考虑容错这个事（高可用），高性能不太考虑这个事

55、ResNet文本领域不行  

56、kaggle上有多gpu，每周30h

57、batch_size变大收敛慢，所以要调高学习率，调高学习率导致抖动？？，在训练效果差不多的情况下，batch_size越小越好呗。变大的话有很很多无效的特征

58、forward的时候是没有办法并行的（要等着），但是backward的时候，每过一层就可以发一个梯度了（并行去发）

59、假设一个分类问题是n个类别，那么你的batc_size 不要超过 10 * n 或者 20 * n

60、增广，图片颜色增50%，减50%，均值是不变的，数据分布是不变的

61、神经网络的损失函数是一个凸函数，但是只要你加了隐藏层了，那整体就是非凸的，所以最后loss凸不凸没啥用。神经网络通常不太会考虑凸或非凸的情况

62、weight decay 和 lr decay区别在哪，前面是一个正则，防止过拟合，优化算法参数的。。后面是一个超参数，做训练收敛用的

63、cv的话就去看看opencv把，里面的算法

64、标注数据也是一门活，但是会慢慢变成自动的

65、分类问题是有置信度的，比如softmax，而锚框是一个回归问题，没有置信度一说。真实的边缘框是读出来的（手动标记），训练的时候是有真实边缘框的，预测的时候没有真实边缘框

66、denseNet在模型上进行合并，Adam可以作为别的优化算法，学习率基于cosine 或者 训练不动时往下调

67、CV玩剩下的nlp可以拿来用？？还是互相成就

68、transformer+attention的效果目前可以替代掉cnn，cnn在局部表现的非常好，而trans+atten在全局表现的非常好

69、某一个卷积层的输出就叫作特征与（feature map）

70、多尺度，就是同一次训练中，每个block/stage用不同的锚框大小，定义不同的网络层，有base_net，download_sample_blk等等，约靠近输入，分辨力是越大的，看到的局部信息越多，越远离，分辨小，但是看全局

71、目标检测 应该也是拿图片分类那部分的模型做 pre-train，每个像素做锚框是每个feature map上做锚框，不是输入上，backward只能有一个值回传，所以不同的loss最后要合在一起

72、图片分割一般是crop的size，而不是用resize，因为像语义分割等，需要标号的，resize后标号不知道插什么值（不是training）

73、语义分割的数据集要比图像识别小很多，标起来很贵的，标一张医学图像50

74、语义标注，labelme，飞浆paddle、字节、58的众包项目

75、自动驾驶，很多种cv模型配合去完成，特拉斯用的纯视觉，国内会用激光雷达、国外国内都会用摄像头，国内用的雷达贵

76、内容信息，越靠近图片，也就是下面，还原度越高

77、卷积一般是做下采样、转置卷积一般是做上采样，转置卷积可以通过padding和stride变化输入和kernel，然后再做卷积实现

78、整个rnn都在处理一个序列信息，tau是不是越大越好，理论上是的，但也不能太大，假设时序的tau就是全量值了，那训练的样本不就只有1个了么

79、潜变量模型是可以使用隐马尔科夫假设的，两者之间没有太多关联

80、tau取的比预测的点数多一点也行，取tau是一个比较trick的事

81、有些场景下要有足够多的负类样本，正类学到的没什么用也。。

82、用cnn去处理时序数据的话呢。。理解成一个1D的数据，做1dconv

83、中文分词比较好用的 jieba

84、2018年transform出现，nlp得到大发展，就是刷pre-train的模型，就是backbone，骨架刷好（需要一个团队，数据集），个人就是用transform去做应用

85、之前说用cnn做图片，用rnn做文本，现在说transform都能做。。做多模态

86、gpt的模型，内存是一个巨大的瓶颈

87、gru中的激活函数可以不用tanh，用relu啥的，relu是后来出的

88、nlp比图片好work一点，文本要远远大于图片，聊天，写文章，读书、代码、但文本翻译目前比较成熟

89、embedding是one-hot降维后的一个浅空间

90、现在seq2seq都用transformer实现了，RNN和LSTM使用场景呢，确实transformer是潮流。循环往复，螺旋上升。

91、21年就提到了openai了

92、PyTorch 在 2022 年后加入了对 Apple GPU 的支持。有些算子（比如部分自定义卷积或复杂张量操作）暂时不支持 MPS，会自动 fallback 到 CPU

93、为什么可以用很大的硬件去训练自注意力机制？ 因为并行度很好，适合这样的并行运算。自注意力机制适合处理比较长的句子，它的设计使它看的东西比较宽，看的长也有代价，计算复杂度比较高

94、transformer的一大好处就是可并行，本身没有太大的硬件需求，但是 bert和gpt就是需要的

95、kvq的大小一般就是hidden_size

96、transformer处理图像，就是把一个个块抠出来当作一个序列

97、transformer论文中多头注意力机制是为了学习cnn的多通道输出，这个多头解释的不错了

98、如果是一个很大的负数输入到softmax的话，结果是一个0

99、多头注意力是有4个W可以学习的（缩放点积attention的话），transformer里面编码器和解码器的中第一层的 attention的输入kvq是一样的，自注意力机制，而编码器往解码器发的那个attention不是自注意力，解码器中的第二个attention的key和value是来自编码器的输出，而query来自解码器第一个attention的输出

#### 论文

AlexNet

https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf



#### need check


5、背景里面的44再回顾一下，crossEntropy自带softmax

6、30节有第二个竞赛，分类图片的

7、34节的多gpu计算

8、https://cv.gluon.ai/model_zoo/detection.html

9、感受野再看一下

10、上采样和下采样再看下（卷积和转置卷积）

11、马儿卡夫链 和 傅里叶变化


13、把rnn的实现自己过一遍 55节，嗯，各种shape啥的



15、d2l文件中的dataloader没有+ encoding='utf-8'

16、svm的实现还是看看

17、64 里面有加了注意力机制的监控点图，可以模拟atp？？比较有用，加入注意力机制，给定成对的输入-输出，符合学习f来预测任意新的x的输出h_hat，可以有点把线切进来的意思

18、asic

19、把rnn到transformer那块看下，代码调下

20、吴恩达的对话系列



22、2012年alex用的 gtx580，3G的内存，比方alexnet论文里体现的，压缩到最后一层，4096的向量，已经能让机器很好的表述一张图片了，那么相近的向量可以理解成是一个东西，多伦多大学

23、transformer，attention is all you need 是2017年的nips，google的人写的，还有实习生，把rnn去掉了，全部使用的是attention

24、算法岗面试常考点总结一下就行，layer norm 和 batch norm

25、kernel大小为1的卷积，transformer里面的前馈全连接层是对一个词的mlp，attention负责横向沟通，抓住上下文关系，FFN，负责纵向加工，增强每个token的表示，self-attention 和 attention 怎么讲明白，transformer里面就是很多大的矩阵做乘法，设置softmax的概率为0.1。。就是0.1就算分类成功了，perplexity困惑度会降低，但是精度和bleu值会提高，停下transformer论文LM的最后总结

26、vit论文精讲的那个网络分析一定要看下！！！太细了，多头怎么降维的

27、Clip里面有个伪代码 讲多模态的，可以再细看下

28、cross attention的输入序列是两个，一个当做K，V，一个当做Q，学习句子间的相关性

29、看看llama3 的模型部分的 改进kv cache的部分，这篇论文对比llama2 基本就是4个点的改变，都可以看一下

30、2025年比较多的 MOE

31、数据采样的部分（一些算法）

32、PPO，强化学习相关可能是要看下的

33、文生视频，开源的有hunyuan，闭源的有hailuo ai 和 kling ai

34、github上面一些面试的题目

35、rag、langchain、prompt、workflow

36、deepseek

37、：FAISS、Chromadb、ES、milvus等（向量数据库，rag的入库）

38、常见的数据检索方法包括：相似性检索、全文检索等，根据检索效果，一般可以选择多种检索方式融合，提升召回率。相似性检索：即计算查询向量与所有存储向量的相似性得分，返回得分高的记录。常见的相似性计算方法包括：余弦相似性、欧氏距离、曼哈顿距离等。全文检索：全文检索是一种比较经典的检索方式，在数据存入时，通过关键词构建倒排索引；在检索时，通过关键词进行全文检索，找到对应的记录。

39、sklearn要看下

40、把langchain实现一下

41、决策树模型看看

45、gan里面，说到不用markov去采样，diffusion好像就是利用markov链的那种

46、beam search

47、chatglm(中文语料)，了解中文模型的特点

48、本地知识库中，除了ollma去拉一个llm外，还有个嵌入模型，用来上传文本，做本地rag知识库，用来进行问答

49、dify去做工作流，工作流拿一个网页，然后去做个思维导图的展示。不同节点通过判断分支，然后有结束节点，然后有模型节点这样。

50、hugging-face 相关的 transformer包

51、量化部署看下

52、lora的 微调

53、diffusion model check一下

54、llama的rope方法处理位置看一下，训练的

55、词向量相关的方法

56、kv cache

57、用quantization（量化） 载载模型，

58、Lora rank，有论文比对dropout、weight decay

59、布隆过滤器

60、beam search

61、困惑度

62、大模型存在的问题，注意力池（attention sink），即少量特殊的token计算中产生很大的输出值，占据很高的注意力分数。巨量激活，即模型激活中出现大于中位数数千倍的离群值

#### confirm

1、bp，反向传播其实就是利用了链式法则，，求loss对于某个w_1的偏导数（如果我们想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出），这边求导就用到了链式法则，这里面包含了激活函数等等，所以要采用链式法则。那么一次反向传播就完成了，更新权重的方式是 w - lr * (求出的偏导数)。

2、梯度下降（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集（所有样本进行一遍前向传播，得到output再去算损失），因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。梯度下降去优化目标函数的原因是啥，因为loss要越小越好，0最好了，那么就是到函数的极值，极值的点是不是导数为0.。那么偏导数也是一个意思，就是让对参数的偏导（梯度）为0。。注意是偏导，不是w。然后w是根据梯度的反方向去更新的！因为梯度指向的是你的值变大的方向，所以要减梯度，朝着值变小的方向。收敛就是梯度近乎等于0

2_1、反向传播是用来算梯度的（运用链式法则，可以看手搓的例子，用整体的误差对某个w求偏导，运用的就是链式法则，先前向传播算出总误差，再链式找到w，就能算出梯度，这边需要注意的是反向的时候当w不是在最后一层的时候，其实它会受到很多下游神经元的影响，也就是链式法则中有+号的存在，总之能算），梯度下降负责用这些梯度更新参数（w -> w - yita * 梯度）

3、对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连，我们将这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）

4、23节的LeNet可以跑，画图行，稍微改下，mac的gpu跑的话，也小改下就行了。M4芯片两个不同的跑法 7569.1 examples/sec on cpu，37697.3 examples/sec on mps（不是一个量级，但没有彪的那么快。。）

5、这个用了24节的AlexNet（两个巨大的全连接层，有将近1GB的参数），跑的不是imageNet，跑的fashion-mnist。mac电脑直接起飞了，用的mps，风扇狂转。。电脑发热。GPU监控拉满。视频里面跑是3min半（3897.5 examples/sec on gpu(0)），mac上跑了12min，loss 0.333, train acc 0.879, test acc 0.879，739.7 examples/sec on mps。跑完后mac还有余热。。

6、kaggle的竞赛，流程走过一遍了，代码还是要改改的（pd底层np的问题，无法直接转换成torch），在kaggle上有autogluon的用法

7、读论文，AlexNet（deep启蒙），

8、注意力相关，自注意力、cross attention、bi self attention、causal self attention（因果关系注意力）

9、pr一把dl相关的

10、openai的API全部熟悉了一遍，gpt5版本，fine-tuning模型（把prompt，answer的pair喂给它），蒸馏模型，top-k采样和temperature。流输出（自己for循环去提取text）。presence_penalty 和 frequence_penalty，前者 是一个在-2.0到2.0之间的数字，如果是正的，模型会更多地推荐新的内容，因为曾经被使用过的内容会收到一个惩罚。后者 是一个在-2.0到2.0之间的数字，如果是正的，模型就不太可能重复推荐曾经用过的内容。n可以控制输出的结果数量。stop可以选择参数进行停止补完。https://cookbook.openai.com/ 之前ZH中的内容源自这里<-，哈哈。文本嵌入功能，度量两个文本字符串有多么相似。openai.Embedding.create() 和 get_embedding。两者类似，但是前者返回包含嵌入结果的JSON，而后者直接返回嵌入结果的列表，在dataframe中更加实用。上下文和记忆，将历史的内容塞到新问题前面，但是本身token有限制且history过多会导致费用过高，可以后进先出的记忆（LIFO）。最近的提示词肯定不妥，可以使用选择性上下文。计算最符合的N的交互内容并返回。

聊天API，openai.ChatCompletion.create，必须有模型和messages（role 和 content），system/assistant/user。system的name可以设置成example_user 和 example_assistant，用来进行提示词样例。

#### prompt engineering

提示词的一些优化，

1、提示词以固定的分隔符结尾，补全以空格开始

2、用多一点例子去做样本

3、输入数据用自然语言可以优化性能，尤其对应生成式模型，而不是structed的数据

4、在fine-tuning的时候也可以创建验证集，validation_data.json。openai api fine_tunes.create -t train_data.jsonl -v validation_data.jsonl -m <engine>，也有超参数的设置，n_epochs和batch_size，学习率。compute_classification_metrics，这个指标适用于分类任务的精调。如果设置为True，每个epoch的结束，它会在验证集上计算分类相关的指标（比如准确度和F1得分）。这个指标可以帮助你评估模型的表现，做出调整。

5、如果是分类问题，可以把类别的string换成数字1-10等等，减少token使用量，训练的数据量变小，推断的token数量也只有1个

6、api会返回给你用量的 usage。包括提示词 和 返回的。"usage": {"completion_tokens": 7,"prompt_tokens": 4,"total_tokens": 11}，输出的长度用max_token限制一下

7、1个token大约4个字符文本，100个token大约75个单词

8、\n\n 后面接目标 \n\nkeyword：

9、生成待办清单，

```html

next = openai.Completion.create(
  model="text-davinci-002",
  prompt="Todo list to create a company in US\n\n1.",
  temperature=0.3,
  max_tokens=64,
  top_p=0.1,
  frequency_penalty=0,
  presence_penalty=0.5,
  stop=["6."],
)


1. Choose a business structure.
2. Register your business with the state.
3. Obtain a federal tax ID number.
4. Open a business bank account.
5. Establish business credit.

```

10、编辑文本，Api变成Edit，参数从prompt变成input和instruction，指令是必须的，输入是可选的

```html

response = openai.Edit.create(
  model="text-davinci-edit-001",
  input="Hallo Welt",
  instruction="Translate to English",
)

print(response)

```

同样的任务可以用补全接口去完成

```html

response = openai.Edit.create(
  model="text-davinci-edit-001",
  instruction="Translate from English to French, Arabic, and Spanish.",
 input="The cat sat on the mat."
)

next = openai.Completion.create(
  model="text-davinci-003",
  prompt="""
  Translate the following sentence from English to French, Arabic, and Spanish.
  English: The cat sat on the mat.
  French:
  Arabic:
  Spanish:
  """,
  max_tokens=60,
  temperature=0
)

下面一个例子则是反过来，用编辑接口来执行补完的任务：

response = openai.Edit.create(
  model="text-davinci-edit-001",
  instruction="Complete the story",
  input="Once upon a time",
)

```

11、把补全和编辑串联起来使用，先产生一个推文，然后再翻译成西班牙语

```html

english_tweet = openai.Completion.create(
  model="text-davinci-002",
  prompt=prompt,
  temperature=0.5,
  max_tokens=20,
)

english_tweet_text = english_tweet["choices"][0]["text"].strip()
print("English Tweet:")
print(english_tweet_text)

spanish_tweet = openai.Edit.create(
  model="text-davinci-edit-001",
  input=english_tweet_text,
  instruction="Translate to Spanish",
  temperature=0.5,
)

spanish_tweet_text = spanish_tweet["choices"][0]["text"].strip()
print("Spanish Tweet:")
print(spanish_tweet_text)

```

12、不同的单词会有不同的意思，告诉模型上下文，然后再解释

```html

The light is red.
This desk is very light.
You light up my life.

prompt_a = "The light is red. Determine the part of speech of the word 'light'.\n\n"
prompt_b = "This desk is very light. Determine the part of speech of the word 'light\
'.\n\n"
prompt_c = "You light up my life. Determine the part of speech of the word 'light'.\\
n\n"

for prompt in [prompt_a, prompt_b, prompt_c]:
  result = openai.Completion.create(
  model="text-davinci-002",
  prompt=prompt,
  max_tokens=20,
  temperature=0,
)

单独问 Determine the part of speech of the word 'light' 会产生不同的变化

让它学习固定的一些语法，然后进行输出，就是指定了输出格式

prompt = """Input: Bitcoin
Output:
BTC was created in 2008, you can learn more about it here: https://bitcoin.org/en/ a\
nd get the latest price here: https://www.coingecko.com/en/coins/bitcoin.
It's all-time high is $64,895.00 and it's all-time low is $67.81.

Input: Ethereum
Output:
ETH was created in 2015, you can learn more about it here: https://ethereum.org/en/ \
and get the latest price here: https://www.coingecko.com/en/coins/ethereum
It's all-time high is $4,379.00 and it's all-time low is $0.43.

Input: Dogecoin
Output:
DOGE was created in 2013, you can learn more about it here: https://dogecoin.com/ an\
d get the latest price here: https://www.coingecko.com/en/coins/dogecoin
It's all-time high is $0.73 and it's all-time low is $0.000002.

Input: Cardano
Output:\n"""

result = openai.Completion.create(
  model="text-davinci-002",
  prompt=prompt,
  max_tokens=200,
  temperature=0,
)

print(result.choices[0]["text"].strip())


```

只希望模型进行单一的输出，就是规范化模型的输出内容

```html

prompt = """
Input: List all the files in the current directory
Output: ls -l

Input: List all the files in the current directory, including hidden files
Output: ls -la

Input: Delete all the files in the current directory
Output: rm *

Input: Count the number of occurrences of the word "sun" in the file "test.txt"
Output: grep -o "sun" test.txt | wc -l

Input:{}
Output:
"""

result = openai.Completion.create(
model="text-davinci-002",
  prompt=prompt.format("Count the number of files in the current directory"),
  max_tokens=200,
  temperature=0,
)

print(result.choices[0]["text"].strip())

输出是单一的，由于我们需要单一的回答，因此这里的temperature参数为空。我们为模型提供了足够的token来处理输出：

ls -l | wc -l

```

而在上面的例子中import click这个包，可以做悬停了，询问你的输入，你type了list all files in /tmp后就会产生ls /tmp，之后会问你是否执行等等，这些程序里面都能控制

13、数学的话，加上let's think step by step

14、比较不错的提示词结构

```html

# Role and Objective

# Instructions

## Sub-categories for more detailed instructions

# Reasoning Steps

# Output Format

# Examples
## Example 1

# Context

# Final instructions and prompt to think step by step

```

15、搞一个提示词优化多agents，逻辑矛盾提示词agent，结构化输出agent，提示词与样例不匹配agent，重写提示词agent，更新不一致的样例agent

16、决策树（Decision Tree），它是一种以树形数据结构来展示决策规则和分类结果的模型，作为一种归纳学习算法，其重点是将看似无序、杂乱的已知数据，通过某种技术手段将它们转化成可以预测未知数据的树状模型，每一条从根结点（对最终分类结果贡献最大的属性）到叶子结点（最终分类结果）的路径都代表一条决策的规则。决策树还能处理回归问题

17、softmax与交叉熵的关系，softmax只是归一化的一个过程，让输出变成0-1之间的概率，而概率[0.2,0.2,0.6] 和 独热编码的 [0,0,1] 真实标号 之间的损失函数不需要像线性回归那样做均方误差，因为0.2到0也产生了误差，还不小，没有什么意义，我们只想知道类别之间的误差，只要类别的输出远远大于别的类别的输出就行了，所以就引入了交叉熵这个损失函数，真实标号为0的时候值为0，真实标号为1的时候值为-logy_hat，再结合softmax算子与交叉熵节后后 对 o_j做偏导得出的偏导数，就是softmax(o_j) - y_j，是输出（估计值）与真实值（观测值）的差异（梯度）。而这个交叉熵H(P, Q) 和 KL散度也是有关系的。。D(P||Q)，KL散度是衡量两个概率分布差异的非对称度量，推导过程把KL散度写成熵/交叉熵的形式，H(P, Q) = H(P) + D_kl(P||Q)。离散和连续都能推导，交叉熵 = 真实分布P的最优平均码长(熵) + 因为用错模型Q而额外付出的代价（KL），熵是越大越chaos，越小越稳定。机器学习里面最小化交叉熵等价于最小化KL，因为上面公式的推导。H(P)是常数固定。

17_1、对于分类问题，当训练样本的标签用 one-hot 向量 表示时（这是前提），交叉熵损失恰好等于负对数似然（NLL）。因此“最小化交叉熵” 等价于“最小化负对数似然” 等价于 “最大化对数似然”。公式中其实有两个加和，内层的加和是类别q，从类别的乘积转变过来。。

18、接上面，当训练样本的标签用one-hot向量表示时（注意这是前提），交叉熵损失恰好等于负对数似然，因此，最小化交叉熵等价于最大化对数似然！！！最大化对数似然估计是数理统计中通过样本观察总体的一个方法，说明概率更好，更准确。而其就自然转换成了最小化交叉熵（是因为本质要最大化似然，但是最大化似然不太好弄（求解），就借助one-hot转而去求交叉熵了。。应该是这样。。）。

19、一般带正则项，约束项的目标方程，其实是用了拉格朗日乘子定理，就是在K个约束的情况下求n个变量的优化问题，用拉格朗日乘数法就可以转换成n+k个变量的方程组，称作拉格朗日方程，这个方程组的解将包括所有最优化问题的解。拉格朗日乘数法所得的临界点会包含原问题的所有临界点，但并不保证每个拉格朗日乘数法所得的临界点都是原问题的临界点。

20、deepseek或者千问中的 moe，moe要了解一下，两个东西定义了moe，experts，是ffnn（前馈神经网络，也就是MLP，另外，GPT、Bert中的transformer block中也有FFNN，是一个二维扩展版的MLP），router（gate network）哪个tokens被发送给哪个专家。transformer的decoder的block中有一个FFNN（dense model，所有神经元都激活），把dense这个切成很多块（也就是很多专家experts）。在某个时间只激活一个专家的子集。这叫做稀疏模型。在推理的时候，只有指定的专家被使用。每个专家也是独立的FFNN。router去选择expert，两者结合在一起，形成了moe layer。等于特么替换掉了之前了FFNN层。router里面就是ffnn加一个softmax。选择某个expert的概率。再加入随机高斯噪音防止一个expert老是被选择（keepTopK）。一般的输入需要乘上router weights。然后加入噪声是防止老是选择同样的专家，这就是负载均衡，更进一步提升负载均衡，我们可以增加辅助的loss，load balancing loss。还有每个专家的容量（工程化的思想？？），超出容量的移到下一个概率比较大的expert。如果都超额了，就直接送到下一层了。MOE比较吸引人的就还有它的计算消耗，还是要将整个网络加载到device，但在推理的时候，只会用一个子集（激活的parameters）。用了mixtral 8*7b做例子，这个模型用了moe layer。就是说它有8个专家，每个有7billion 的param。

第一层，embeddings  32000 * 4096 = 131072000，embeddings是模型的很小的一部分组成，这些是shared param。第二层，masked self-attention，32 * 41943040 = 1342117280，32是repeated decoder blocks，然后41943040是(q, k, v)。结果是shared param。第三层，router，8 * 4096 = 32768。8是experts，结果是shared param。第四层，experts，8 * 5637144576 = 45097156608。8是expert，结果是total param。但如果模型只是同时激活了两个专家，那么整个参数是 11274289152。第五层，LM head，也有少部分参数，32000 * 4096 = 131072000。moe的优势，激活的参数会比较少，interence的时候会更快。对vision model 也可以这样做的moe。就是会做在encoder中的FFNN，因为图片，为了使得capacity比较低，对每个patch有 priority scorer。然后最重要的patch就被processed。创建一些soft-moe，输入的X要乘上一个可学习矩阵。

21、
