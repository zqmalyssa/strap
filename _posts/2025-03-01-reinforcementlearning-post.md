---
layout: post
title: reinforcementlearning
tags: [reinforcementlearning]
author-id: zqmalyssa
---

关于强化学习部分，还是单独开一栏总结一下

#### 基本概念

强化学习和监督学习的区别如下。

（1）强化学习输入的样本是序列数据，而不像监督学习里面样本都是独立的。

（2）学习器并没有告诉我们每一步正确的动作应该是什么，学习器需要自己去发现哪些动作可以带来 最多的奖励，只能通过不停地尝试来发现最有利的动作。

（3）智能体获得自己能力的过程，其实是不断地试错探索（trial-and-error exploration）的过程。探索 （exploration）和利用（exploitation）是强化学习里面非常核心的问题。其中，探索指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们“一无所有”；利用指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。因此，我们需要在探索和利用之间进行权衡，这也是在监督学习里面没有的情况。

（4）在强化学习过程中，没有非常强的监督者（supervisor），只有奖励信号（reward signal），并且奖励信号是延迟的，即环境会在很久以后告诉我们之前我们采取的动作到底是不是有效的。因为我们没有得 到即时反馈，所以智能体使用强化学习来学习就非常困难。当我们采取一个动作后，如果我们使用监督学习，我们就可以立刻获得一个指导，比如，我们现在采取了一个错误的动作，正确的动作应该是什么。而在强化学习里面，环境可能会告诉我们这个动作是错误的，但是它并没有告诉我们正确的动作是什么。而且更困难的是，它可能是在一两分钟过后告诉我们这个动作是错误的。所以这也是强化学习和监督学习不同的地方。

通过与监督学习的比较，我们可以总结出强化学习的一些特征。

（1）强化学习会试错探索，它通过探索环境来获取对环境的理解。

（2）强化学习智能体会从环境里面获得延迟的奖励。

（3）在强化学习的训练过程中，时间非常重要。因为我们得到的是有时间关联的数据（sequential data）， 而不是独立同分布的数据。在机器学习中，如果观测数据有非常强的关联，会使得训练非常不稳定。这也是为什么在监督学习中，我们希望数据尽量满足独立同分布，这样就可以消除数据之间的相关性。

（4）智能体的动作会影响它随后得到的数据，这一点是非常重要的。在训练智能体的过程中，很多时 候我们也是通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。我们通过数据来训练智能体，如果数据有问题，整个训练过程就会失败。所以在强化学习里面一个非常重要的问题就是，怎么让智能体的动作一直稳定地提升。

强化学习是有一定的历史的，早期的强化学习，我们称其为标准强化学习。最近业界把强化学习与深度学习结合起来，就形成了深度强化学习（deep reinforcement learning），因此，深度强化学习 = 深度学习 + 强化学习。我们可将标准强化学习和深度强化学习类比于传统的计算机视觉和深度计算机视觉。

对标传统计算机视觉和深度计算机视觉，其实都是有脉络可循，基本还是把特征提取的部分融合到了网络中来（就是省去了特征工程）

标准强化学习：比如 TD-Gammon 玩 Backgammon 游戏的过程，其实就是设计特征，然后训练价值函数的过程。标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，我们就可以通过训练一个分类网络或者分别训练一个价值估计函数来采取动作。

深度强化学习：自从我们有了深度学习，有了神经网络，就可以把智能体玩游戏的过程改进成一个端到端训练（end-to-end training）的过程。我们不需要设计特征，直接输入状态就可以输出动作。我们可以用一个神经网络来拟合价值函数或策略网络，省去特征工程（feature engineering）的过程。

强化学习能发展起来的缘由和深度学习一样，有了更多的 GPU，可 以更快地做更多的试错尝试。其次，通过不同尝试，智能体在环境里面获得了很多信息，然后可以在环境里面取得很大的奖励。最后，我们通过端到端训练把特征提取和价值估计或者决策一起优化，这样就可以得到一个更强的决策网络。

强化学习的智能体中，有一个或多个如下的组成部分：

1、策略（policy）,智能体会用策略来选取下一步动作，是一个动作模型，它其实是一个函数，把输入的状态变成动作，策略分为随机性策略 和 确定性策略，随机性策略，pai(a|s)，输入一个状态s，输出一个概率。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。比如0.7的概率往左，0.3的概率往右，那么通过采样就可以得到智能体将采取的动作（可能性都可能被采样到，概率会进行学习）。确定策略，是智能体直接采取最有可能的动作max（这个是不会变化的，最有可能的动作）。随机性会更好一点，可以探索环境

2、价值函数（value function），其值是对未来奖励的预测，我们用它来评估状态的好坏，折扣因子，尽可能短的时间里得到尽可能多的奖励，期望E_pai，pai函数的值可反应在我们使用策略pai的时候，到底可以得到多少奖励，还有一种价值函数Q，Q函数里面包含两个变量，状态和动作。所以我们未来可以获得奖励的期望取决于当前的状态和当前的动作，Q函数是强化学习里面要学习的一个函数，当有Q函数后，进入某个状态要采取的最优动作可以通过Q函数得到。

3、模型（model），模型决定了下一步的状态，下一步的状态取决于当前的状态以及当前采取的动作，它由状态转移概率和奖励函数两部分组成，当有了策略，价值函数和模型后，就形成了一个马尔科夫决策过程

根据智能体学习的事物不同，我们可以把智能体进行归类。基于价值的智能体（value-based agent）显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。基于策略的智能体（policy-based agent）直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了演员-评论员智能体（actor-critic agent）。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作.

价值base的比较好用在离散、不连续环境下，策略base的可以用在连续场景下

有模型强化学习 和 免模型强化学习，也就是是否需要对真实环境建模。有模型在虚拟和真实世界中学习，免模型直接与真实环境进行交互学习。一个有模型的强化学习方法也可以在免模型的强化学习方法中使用。一般都采用免模型的方法

测试智能体在 Gym 库中某个任务的性能时，出于习惯使然，学术界一般最关心 100 个回合的平均回合奖励。对于有些任务，还会指定一个参考的回合奖励值，当连续 100 个回合的奖励大于指定的值时，则认为该任务被解决了。而对于没有指定值的任务，就无所谓任务被解决了或没有被解决。

总结一句强化学习：智能体可以在与复杂且不确定的环境进行交互时，尝试使所获得的奖励最大化的算法。环境和奖励函数不是我们可以控制的，两者是在开始学习之前就已经事先确定的。我们唯一能做的事情是调整策略，使得智能体可以在环境中得到最大的奖励。另外，策略决定了智能体的行为，策略就是给一个外界的输入，然后它会输出现在应该要执行的动作。用深度学习的方法去学习一个策略？？pi 就是一个参数为sita的网络（network）（用一个RNN去处理它？？），比如让agent看到什么样的画面（machine输出的）是由你决定的。那么input就是游戏的画面，output就是你有哪些选项可以去执行。行为有3个，output layer就有几个neuron。根据最后输出的概率，决定action。一场游戏就是一个episode。

policy gradient 就是maximize expected reward。穷举所有的trajectory，用它们各自发生的概率pi（上面带sita参数的）乘以得到的总体奖励R，就是目标函数。要让它越来越大，所以不是gradient decent，而是gradient ascent。update参数的时候从减变成加。这里的R函数不需要可微（differentiable）。通过求梯度，介个期望公式，总体与sampel之间的期望关系等，就能算出梯度，然后再去update那个sita。update就是梯度下降的方法，-号变成+号。。用adam或者rmsprop等等。

#### 马尔可夫决策过程

MDP(Markov decision process)，马尔可夫决策过程是强化学习的基本框架，马尔科夫性质，未来状态的条件概率分布仅依赖于当前状态，如果某一过程满足马尔科夫性质，那么未来对的转移与过去的是独立的，它只取决于现在。马尔可夫性质是所有马尔可夫过程的基础。马尔可夫过程是一组具有马尔可夫性质的随机变量序列。离散时间的马尔可夫过程也称为马尔可夫链，马尔可夫链是最简单的马尔可夫过程，其状态是有限的。有限的话可以搞出一个矩阵，值是条件概率，每个s到其他s的条件概率。给定一个状态转移的马尔可夫链，可以对链进行采样，就会得到一串轨迹。。通过对状态的采样，我们可以生成很多这样的轨迹。MRP马尔可夫奖励过程是在链上加了奖励函数。奖励函数R是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励

看回报和价值函数，回报越往后，折扣因子产生的折扣就越大，0就是只关注当前了，1就是没有折扣了，折扣因子是强化学习智能体的一个超参数。奖励函数（reward function）可以定义成智能体进入第一个状态s_1的时候会得到5的奖励，进入第七个状态的时候会得到10的奖励，进入其他状态没有奖励。用向量表示奖励函数，即R = [5,0,0,0,0,0,10]，有了折扣因子和奖励函数向量，就可以计算每一个轨迹的奖励G（回报）。有了一些轨迹的实际回报后，怎么计算它的价值函数？比如想知道s_4的价值，可以把s_4的很多轨迹都采样出来，计算回报，然后取均值就是价值，叫蒙特卡洛采样方法。也可以采取另一种计算方法，从价值函数中推导出贝尔曼方程，有动态规划相关的。V(s) = R(s) + 未来奖励的折扣总和。贝尔曼方程 其中有一步是将G_t+1 转换成 V(S_t+1)，就是将其转换到状态空间中，这样期望就能用可数的状态去求了。贝尔曼方程可以直接得到解析解。。但是有个矩阵求逆的过程，时间复杂度是O(N3平方)，当状态特别多的时候，不如10个状态到1000个，到100万的时候，状态转移矩阵就是100万乘100万的矩阵，这个大矩阵求逆是非常困难的。这种通过解析解去求解的方法只适用于很小量的马尔可夫奖励过程

计算马尔可夫奖励过程价值的迭代算法，将迭代的方法应用于状态很多的马尔可夫奖励过程（large MRP），如动态规划，蒙特卡洛方法，时序差分学习（动态规划和蒙特卡洛的一个结合）。

马尔可夫决策过程相比马尔可夫奖励过程就是多了决策（决策是指动作）， 条件概率里面加了a（action）。

动态规划适合解决满足 最优子结构 和 重叠子问题两个性质的问题，最优子结构意味着问题可以拆分成一个个小问题，通过解决小问题再组合答案可以得到原问题的答案。重叠子问题意味着子问题出现多次，并且子问题的解决能够被重复使用，我们可以保存子问题的首次计算结果，在再次需要的时候直接使用。马尔可夫决策过程是满足动态规划要求的。在贝尔曼方程里面，我们可以把它分解成递归的结构。当我们把它分解成递归的结构的时候，如果子问题的子状态能得到一个值，那么它的未来状态因为与子状态是直接相关的，我们也可以将之推算出来。价值函数可以存储并重用子问题的最佳的解。动态规划应用于马尔可夫决策过程的规划问题而不是学习问题，我们必须对环境是完全已知的，才能做动态规划，也就是要知道状态转移概率和对应的奖励。使用动态规划完成预测问题和控制问题的求解，是解决马尔可夫决策过程预测问题和控制（预测是给定一个策略，求价值函数，控制是不知道策略，给定同样的条件下，求最优的价值函数和最优策略）问题的非常有效的方式。

我们再来对比策略迭代和价值迭代，这两个算法都可以解马尔可夫决策过程的控制问题。策略迭代分两步。首先进行策略评估，即对当前已经搜索到的策略函数进行估值。得到估值后，我们进行策略改进，即把 Q 函数算出来，进行进一步改进。不断重复这两步，直到策略收敛。价值迭代直接使用贝尔曼最优方程进行迭代，从而寻找最佳的价值函数。找到最佳价值函数后，我们再提取最佳策略。

#### 一些背景

基于LHY的课的 learn内容：

- policy gradient (PG)中介绍了很多基本的东西，3个组成部分，演员（actor）、环境、奖励函数。演员的策略pi决定了执行的动作，这个策略是一个网络，里面有一些sita，是可以学习的。在一场游戏里面，我们把环境的输出s和演员的输出动作a
全部组合起来，就是一个轨迹 tao = {s_1, a_1, ... ... s_t, a_t}，给定演员的sita，我们就可以计算某个tao发生的概率为 p_sita_tao = p(s_1)p_sita(a_1|s_1)p(s_2|s_1,a_1)p_sita(a_2|s_2)p(s_3|s_2,a_2)...这个式子可以用连乘符号表示，p_sita_tao后面算梯度的时候可以用来推导。一次s和a下来就有r，把轨迹中所有的r加起来就是R(tao)，需要做的就是调整sita的值，使得R(tao)越大越好，实际上R(tao)也不是一个标量，它是一个随机变量，能计算的是R(tao)的期望值，R_sita_ba = R(tao)p_sita(tao) 对所有tao求和就是期望了，因为我们要让奖励越大越好，所以可以使用梯度上升（gradient ascent）来最大化期望奖励。要进行梯度上升，我们先要计算期望奖励R_sita_ba的梯度，对R_sita_ba做梯度运算，再运用log对数f(x)公式，及一系列的转换，得到简单的值，就能算得梯度，算得梯度后，用梯度上升的办法更新sita。计算梯度的式子直观看就是在采样的数据中，s_t 执行 a_t后发现奖励是正的，我们就要增加在s_t执行a_t的概率。反之也成立。注意这时候的R(tao)在一个tao中是固定的。数据的采样就是玩游戏就行(获得训练数据)，一个tao出很多(s, a)对，一个R，代入公式，算梯度，就可以更新模型了。输入的是s，输出的是a，那么其实也算是分类问题，最小化交叉熵，也是最大化对数似然。强化学习和分类问题唯一不同的地方就是前面乘上一个权重（一场游戏的总权重）。pytorch就可以自动求梯度了。求梯度也有些技巧，添加基线（b，可以用R的期望去算） 和 分配合适的分数（看s_t后面的r的和， 也就是把R(tao)换成当前状态及后面得到的r的和，再在未来的奖励中加一个折扣gama）。把R-b这一项称为优势函数A_sita（advantage function）。优势取决于s和a，优势函数是相对的好，优势函数通常可以由一个网络估计出来，这个网络被称为评论员（critic）。蘑菇书中有讲PG跟神经网络的结合，实际的动作就是一个a，但它不一定正确，只能依靠奖励去判断，奖励越大，这个真实动作就越好。
- reinforce 算法是基于策略梯度的强化学习经典算法，采用回合更新（就是一轮结束后更新一次，会比较慢mc 和 td中的mc方法）的方式
- policy gradient 引申出来的 PPO（加KL散度） 和 PPO2（加min的 1 + epoxilo），在强化学习里面，要学习的是一个智能体。如果要学习的智能体和与环境交互的智能体是相同的，我们称之为同策略。如果要学习的智能体和与环境交互的智能体不是相同的，我们称之为异策略。为什么我们会想要考虑异策略？让我们回忆一下策略梯度，策略梯度是一个同策略的算法。演员去与环境交互搜集数据。一旦更新参数，sita变成sita_orther，那么p_sita(tao)就不对了，之前采样的数据也不能用了，所以策略梯度是会花很多时间来采样数据的算法，其大多时间都在采样数据，智能体与环境交互后，接下来就是更新参数，又只能更新一次参数，然后就要重新采样数据，太花时间了，所以想从同策略变成异策略。用另一个策略去与环境互动，被sita_fi采样到的数据去训练sita。就可以多次使用采样到的数据，可以多次执行梯度上升，可以多次更新参数，都只需要同一批数据。这边用到的技术就是重要性采样（importance sampling）。用另一个分布q来做修正，从q采样出来的每一笔数据，都需要乘一个重要性权重（importance weight） p(x) / q(x) 来修正 p 和 q之间的差异。q(x)可以是任何分布，唯一的限制就是q(x)的概率为0的时候，p(x)的概率不为0，不然会没有定义？？？p和q的差距也不能太大，用variance的公式推导一下，如果p和q差距很大，那么方差就会很大。有另外一个演员，它的工作是做示范用，sita_fi是为sita做示范使用（demonstration）。tao是从sita_fi采样出来的。重要性权重就是某一个轨迹tao用sita算出来的概率除以这个轨迹用sita_fi算出来的概率。最后推导的式子，计算是方便的。通过重要性采样，把同策略换成异策略，但是两个分布相差太多的话，重要性采样的结果就会不好，怎么避免它们相差太多呢，这就是PPO要做的事，多加了一个约束（constrain），这个约束是sita和sita_fi输出动作的KL散度（KL divergence）。虽然PPO优化目标涉及到了重要性采样，但它只用了上一轮策略sita_fi的数据，PPO的目标函数中加入KL散度的约束，行为策略sita_fi 和 目标策略 sita 非常接近，PPO的行为策略和目标策略可以认为同一个策略，因此PPO是一个【同策略算法，因为加了KL散度，使得两个策略很像，远了我就罚你的分】，相比TRPO，PPO直接把约束放到了优化的式子里面，就可以用梯度上升去最大化式子。PPO实现起来简单很多。 KL 散度并不是参数的距离，而是动作的距离。PPO 算法有两个主要的变种：近端策略优化惩罚（PPO-penalty）和近端策略优化裁剪（PPO-clip）。正则就是告诉优化器，别跑太远。PPO是on-policy的(也就是上面说的同策略on-policy和异策略off-policy)。
- 请问同策略和异策略的区别是什么，即生成样本的策略（价值函数）和网络参数更新时的策略（价值函数）是否相同。
- q learning（off-policy的）（其实是讲的DQN，dqn就是只critic，不是actor-critic），传统的RL会用表格存储状态价值函数V(s)或者动作价值函数Q(s, a)。而状态空间一般是连续的，不能用表格去存储了，用价值函数近似（value function approximation）直接拟合状态价值函数 或 动作价值函数。Q是个神经网络，称为Q-network。DQN是深度Q网络，是指基于深度学习的Q学习算法，主要结合了价值函数近似与神经网络技术，并采用目标网络和经历回放的方法进行网络的训练。DQN是value-based的方法，不是learn policy，是learn一个critic（不直接采取行为），critic评价现在的行为有多好 和 多不好。given an actor pi，it evaluates how good the actor is。这里actor是pi，critic是绑一个actor的，这个critic的output值有多大，一方面取决state，另一方面取决于pi（actor）。state value function就是V_pi(s)。它是一个network。所以评论员的输出值取决于状态和演员。评论员其实都要绑定一个演员，它是在衡量某一个演员的好坏，而不是衡量一个状态的好坏。这里要强调一下，评论员的输出是与演员有关的，状态的价值其实取决于演员，当演员改变的时候，状态价值函数的输出其实也是会跟着改变的。如何估计V_pi(s)，蒙特卡洛（MC）based approach（需要游戏玩到结束）。还有一个是temporal-difference(TD) 方法（时间差分）。td不需要玩完一整个游戏，只要有s_t，a_t，r_t，s_t+1 就行了。mc based的G_a会有larger variance（一整个游戏所有的reward合起来）。而td的r是某一个r，它的variance会比较小。而td的r又会估的不准，所以td和mc各有优劣，还有综合版本，td的方法目前比较常见。另一种critic，state-action value function Q_pi(s, a)（Q函数），这边input不像之前只有s，是s 和 a。看到s的时候强制采取a。虽然我们学习的Q函数只能用来评估某一个策略pi的好坏，但只要有了Q函数，我们就可以进行强化学习，就可以决定要采取哪一个动作，就可以进行策略改进。Q learning，有一个初始的pi（一个actor），跟环境互动，会收集数据，接下来learn一个pi这个actor的q value，去衡量一下pi这个actor，它在某一个state强制采取某一个action，接下来用pi这个policy会得到的expected reward（用td 或者 mc都是可以的）。只要learn出一个pi的q function，就能找一个新的policy pi_p，better than pi。如此循环下去，所以为什么能找打更好的policy呢，就是V_pi_p(S) >= V_pi(S) for all state s。第一个技巧是固定网络，就是固定右边的Q，第二个技巧是探索（exploration），当我们使用Q函数的时候，策略完全取决于Q函数。在policy gradient里面a是随机的（stochastic），而在q function中，a是固定的，也会有问题，就是没看过某个 (s, a_1) 出现是估不太出那个值的。但哪天sample到了a_1后，就永远都去挑a_1了，需要exploration一下，不能就挑一个action，解决的方法，epsilon greedy。就是加一点random，一开始随机一点，后面慢慢的就根据Q去选择了。另外一种就是boltzmann exploration。replay buffer里面有不同policy的experience，一个exp里面有s_t, a_t, r_t, s_t+1。rl中跟环境做互动反而比train network要慢，用replay buffer，可以减少跟环境做互动的次数。过去的policy sample到的exp也是可以被利用的。会比较diverse，至于不是要观测pi么，会不会有别的pi_old影响？不太至于，因为exp只包含了一部分，比如s_t, a_t, r_t, s_t+1，它不是完整的trajectory，如果某个算法使用了经验回放这个技巧，该算法就变成了一个异策略的算法。因为本来Q是要观察pi的经验的，但是实际上存储在回放缓冲区里面的这些经验不是通通来自pi的。看算法图就会清晰很多了。而这个算法图才是深度Q网络。深度Q网络 和 Q学习有什么不同？整体来说，深度Q网络与Q学习的目标价值以及价值的更新方式都非常相似。主要的不同点在于：深度Q网络 将Q学习与深度学习结合，用深度网络来近似动作价值函数，而 Q学习 则是采用表格存储。深度Q网络采用了经验回放的训练方法，从历史数据中随机采样，而Q学习直接采用下一个状态的数据进行学习

- 有模型 和 免模型，如果我们知道环境的状态转移概率（0.1 和 0.9）和奖励函数，就可以认为这个环境是已知的，因为我们用这两个函数来描述环境。如果环境是已知的，我们其实可以用【动态规划】算法去计算，如果要逃脱，那么能够逃脱的概率最大的最佳策略是什么（注意，最大最好）。很多强化学习的经典算法都是免模型的，也就是环境是未知的。 因为现实世界中人类第一次遇到熊时，我们根本不知道能不能逃脱，所以 0.1、0.9 的概率都是虚构出来的概率。熊到底在什么时候往什么方向转变，我们通常是不知道的。 我们处在未知的环境里，也就是这一系列的决策的概率函数和奖励函数是未知的，这就是有模型与免模型的最大的区别。策略迭代和价值迭代都需要得到环境的转移和奖励函数，所以在这个过程中，智能体没有与环境进行交互。在很多实际的问题中，马尔可夫决策过程的模型有可能是未知的，也有可能因模型太大不能进行迭代的计算，比如雅达利游戏、围棋、控制直升飞机、股票交易等问题，这些问题的状态转移非常复杂。当马尔可夫决策过程的模型未知或者模型很大时，我们可以使用免模型强化学习的方法。免模型强化学习方法没有获取环境的状态转移和奖励函数，而是让智能体与环境进行交互，采集大量的轨迹数据，智能体从轨迹中获取信息来改进策略，从而获得更多的奖励。

- 悬崖行走问题是经典强化学习问题。 最开始的时候，Q 表格会全部初始化为0。智能体会不断和环境交互得到不同的轨迹，当交互的次数足够多的时候，我们就可以估算出每一个状态下，每个动作的平均总奖励，进而更新 Q 表格。Q表格的更新就是接下来要引入的强化概念。强化是指我们可以用下一个状态的价值来更新当前状态的价值，其实就是强化学习里面自举的概念。在强化学习里面，我们可以每走一步更新一次 Q 表格，用下一个状态的 Q 值来更新当前状态的 Q 值，这种单步更新的方法被称为时序差分方法。

- PPO 是一种 policy-based 算法，确切地说是 “policy-gradient + actor-critic” 算法。它不是从Q(s, a)中找argmax，也不是通过bellman方程更新Q表。优化目标是策略梯度形式。不过PPO也常被称作actor-critic（优势函数A是一个critic网络）（价值函数做baseline）。

- 与基于策略梯度的方法相比，深度Q网络比较稳定，策略梯度比较不稳定，玩大部分游戏不能使用策略梯度。

- 将policy-based的算法 和 value-based的算法结合起来就是actor-critic算法，A3C比较有名。之前的A函数里面变成 Q函数 - V函数的方式就是优势演员-评论员算法了，而Q函数可以被V代掉，这样只要学习V的网路就行了，另外还有个actor的policy网络，而这两个网络的输入又恰好是S，所以前面几层layer可以share，就是cnn抽图像特征那套东西，前面是一张很大的网络。还有个trick就是对pi的输出的分布设置一个约束，这个约束使分布的熵不要太小，也就是不同动作被采用的概率平均一些。而pathwise derivative policy gradient 用到的学习actor去解Q的argmax式子跟GAN很像，Q就是discriminator，在rl里面就是critic，actor在gan里面就是generator

#### need check

deep mind 去发的DQN，玩atari的paper。


#### confirm

1、其实RL去学的policy以前可能是通过表格去look up的，现在就是替换成了network（DL的东西，去学习一个function）。也可以接rnn或者transformer架构。输出要有一些随机性的，sample的。强化学习也是有些参数需要学习，定义目标函数，最大化reward，然后做优化（优化的部分参考PG，就是先算出grad，再去梯度上升方法更新） ？？？ L = e_1 - e_2 +  e_3，把这个+-的想与不想（就是想做这个动作 或者 不想做这个动作，想的就要拉近，不想的就要远离，e看做是误差，训练数据是sample出来的），把想要不想要的+-换成一个分数Score（A），乘到e上就是一个loss了。然后min Loss，期待训练处的参数能符合我们的行为(https://www.youtube.com/watch?v=US8DFaAZcp4 视频中算法确实是这样的，跟正常的监督学习一样了？？)。如果要跑400个epoch，收集数据也是要400次的。然后只能更新一次。the actor to train and the actor for interacting is the same. -> 同策略！！（on-policy，因为要用新的sita再去sample的，不然两个的分布不对）。exploration，让actor去采取不同的action，不然有些action都没用过，怎么知道好不好。有些技术比如enlarge entropy 或者 在输出加噪声来增加随机性。sample出来的a到底是不是正确标是不知道的，所以score（A）就有作用了，乘在前面，当作权重了（所以score A的计算方式有很多变化）。算法步骤：初始化sita，for循环，用sita和环境interact，获得数据{s_1, a_1}，{s_2, a_2}。。计算A_1，A_2，。。。，计算loss，更新sita。

2、critic，G是玩完一场游戏后就能自然得到的，而critic中的value-function，它只有一个输入s，就去拿discounted cumulated reward。V(S)，未卜先知（上面的1不会这样）。value-function的数值跟观察的对象sita也是有关系，有些sita本来就不行，不管S输入的多好，分数也拿的低。critic被训练出来的方式，1个是mc，就是用sita去sample，能够得出看到s_a后得到的reward是多少，G_a，V_sita的输出就要 <-> G_a，越接近G_a就行。2个是TD了，不用玩完去收集G，而是s_t，a_t，r_t，s_t+1的时候就可以进行更新了。因为V_s_t = gamaV_s_t+1 + r_t。有上面的4元序列，并且满足公式，就可以训练value-function。输入模型后的值 相减得到的结果 应该和 <-> r_t 接近。（4元序列里面有r_t）。advantage actor-critic，A_t = r_t + V_sita_s_t+1 - V_sita_s_t。（G - b的期望变形版）。actor 和 crritic的两个网络前面几层可以共享

3、上面两种的话policy 和 critic，网络输入输出不同啊，一个是 s -> action，一个是 s 在sita下到 g(value)

4、sparse reward，reward基本都是0的话。人类帮忙定一些reward，叫做reward shaping。比如游戏的行为会有相应的正负reward。还有给机器加上好奇心，看到新的东西就加分，得是有意义的新（玩马里奥）。如果连reward都没有呢？？imitation learning。expert的示范，tao_hat。还有就是把reward function学出来。inverse RL
