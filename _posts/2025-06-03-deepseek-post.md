---
layout: post
title: deepseek
tags: [deepseek]
author-id: zqmalyssa
---

deepseek 相关

#### 写一些deepseek用到的技术

deepseek-R1 不是从0开始训练的，它从一个相对还不错的LLm，deepseek-V3过来的，他们想让它更加的reasoning。首先尝试的是纯RL，就是deepseek-r1-zero，然后真正的deepseek-r1，通过不同阶段使得其更有条理，给一些起始数据来启动，然后进行RL，然后更多数据，然后更多RL

输入的问题会先经过记忆系统（memory system），该系统通过查找相关信息快速构建上下文。可以将其视为快速回忆你之前遇到过的类似情况。它的主要优势在于其决策系统。在理解你的输入后，它会使用一个智能路由器在两条路径之间做出决定：快速处理器用于简单的任务（如简单问题或常见请求）和专家系统用于复杂的问题（如分析或专业知识）。

This router is what makes the DeepSeek V3 a mixture of experts model (MOE)

简单的问题通过快速路径获得快速、直接的答案，而复杂的查询则通过专家系统得到详细关注。最后，这些响应被组合成清晰、准确的输出。

DeepSeek V3 as the Policy Model (Actor) In RL Setup，也就是说V3在RL中是actor（agent）。

RL 智能体（DeepSeek V3）首先采取一个行动，这意味着它为放入其环境中的给定问题生成一个答案和一些推理。在这种情况下，环境仅仅是推理任务本身。采取行动后，环境会给出一个奖励。 这个奖励就像反馈一样，它告诉 DeepSeek V3 基础模型其行动的效果有多好。积极的奖励意味着它做对了某件事，也许给出了正确的答案或进行了良好的推理。然后这个反馈信号会回到 DeepSeek-V3-Base，帮助它学习并调整未来采取行动的方式，以获得更好的奖励。

所以，有许多强化学习算法可用，但传统的强化学习使用一种叫做“评论家”（critic）的东西来帮助主要的决策部分（“行动者”（actor），即 DeepSeek V3），正如你已经知道的那样。这个评论家通常和行动（critic and actor）者本身一样庞大和复杂，这基本上使计算成本增加了一倍。

然而，GRPO 的做法有所不同，因为它直接从一组行动的结果中确定一个基线，一种良好行动的参考点。正因为如此，GRPO 根本不需要单独的评判模型（critic）。这节省了大量的计算量，使事情更加高效。它始于向模型提出的问题或提示，称为“旧策略”。GRPO 不是只得到一个答案，而是指示旧策略针对同一问题生成一组不同的答案。然后对这些答案中的每一个进行评估并给出奖励分数，反映其好坏或可取程度。GRPO 通过将每个答案与所在组中其他答案的平均质量进行比较来计算“优势”（advantage）。比平均水平好的答案会获得正优势，而较差的答案会获得负优势。至关重要的是，这是在无需单独的批评模型的情况下完成的。这些优势分数随后被用于更新旧策略，使其在未来更有可能产生优于平均水平的答案。这个更新后的模型成为新的“旧策略”，并且这个过程重复进行，迭代地改进模型。

什么鬼，只训练policy是吧？Group Relative Policy Optimization，是一种 改进版的强化学习策略优化算法，常用于大模型（LLM）的对齐训练（类似 PPO，但更稳定、成本更低）。这个算法最早被 DeepSeek、Qwen 等国内外模型在训练中采用

传统的PPO 需要 额外训练一个奖励模型（Reward Model） → 成本高，PPO 的 “clip + advantage” 操作比较复杂，梯度容易震荡。对 batch 中样本依赖大。GRPO的关键想法是 不再训练奖励模型，而是使用一组样本之间的相对排名来直接计算 advantage。（这个可以具体看下）

DeepSeek R1 论文明确提到，在 DeepSeek-R1-Zero 中避免使用神经奖励模型，以防止奖励篡改并降低这个初始探索阶段的复杂性。

有趣的是，DeepSeek 团队有意保持这个模板简单并专注于结构，而不是告诉模型如何推理。

每个输出都将根据正确性和推理质量进行评估并给予奖励。格式是reasoning包含在<think></think>中，而答案包含在<answer></answer>中，为了引导模型进行更好的推理，基于规则的奖励系统开始发挥作用。每个输出都根据以下内容被分配一个奖励：1、准确性奖励：答案是否正确。2、格式奖励：推理步骤是否使用<think>标签正确格式化。

输出 o2 和 o3 获得积极优势，这意味着它们应该被鼓励。输出 o1 和 o4 获得消极优势，这意味着它们应该被阻止。GRPO 随后使用计算出的优势来更新策略模型（DeepSeek-V3-Base），以增加生成具有高优势输出（如 o2 和 o3）的概率，并降低生成具有低优势或负优势输出（如 o1 和 o4）的概率。

更新根据以下内容调整模型权重：1、政策比率：新政策与旧政策下生成输出的概率。2、裁剪机制：防止可能破坏训练稳定性的过大更新。3、KL 散度惩罚：确保更新不会与原始模型偏离太远。（KL的惩罚项）

这确保在下次迭代中，模型将更有可能生成正确的推理步骤，同时减少不正确或不完整的响应。因此，强化学习是一个迭代过程。上述步骤使用不同的推理问题重复数千次。每次迭代都逐渐提高模型在以下方面的能力：1、执行正确的运算顺序。2、提供逻辑推理步骤。3、始终一致地使用正确的格式。

随着时间的推移，模型从错误中学习，在解决推理问题时变得更加准确和有效。（不停迭代）（看blog）

V3模型通过强化学习训练过程创建了DeepSeek-R1 Zero 。它的问题就是压根不限制reasoning的内容，使得可读性特别差。另一个问题就是语言混合，多语言会混合各种语言，西班牙语混合，思考会是英语与西班牙语的混合。

然后就是从DeepSeek-R1 Zero 变成 R1的过程了

So to fix R1 Zero issues and really get DeepSeek reasoning properly, researchers performed a Cold Start Data Collection and included Supervised Fine Tuning.（SFT）

你可以把它想象成在真正激烈的强化学习训练之前，给模型一个良好的推理基础。基本上，他们想教 deepseek-v3 base 什么是良好的推理以及如何清晰地呈现它。

Few shot Prompting with Long CoT，那些 | special_token | 东西只是将推理步骤与摘要分开的标记，使模型可以清楚地学习结构。

Direct Prompting，

Post Processing Refinement，他们甚至使用了已经训练好的 R1 Zero 模型的输出。尽管 R1 Zero 存在问题，但它能够进行一定程度的推理。所以，他们拿了 R1 Zero 的输出，让人类标注员使其更好、更干净、更有条理，并纠正任何错误。

他们最终得到的“冷启动数据”非常好，原因是：1、高质量推理示例：每个示例都展示了良好的逐步推理 2、一致、易读的格式：特殊标记|格式使一切变得统一且易于处理。3、人工检查：他们确保过滤掉任何不良示例，因此数据是干净且可靠的。在获得此冷启动数据后，他们进行了有监督微调（Supervised Fine-Tuning，SFT）。

SFT开始（前面是在搞数据），SFT 第一阶段的核心思想是使用监督学习来教导 DeepSeek-V3-Base 如何产生高质量的、结构化的推理输出。基本上，我们向模型展示了许多良好推理的例子，并要求它学习模仿那种风格。对于 SFT，我们需要将我们的冷启动数据格式化为输入-目标对。对于我们数据集中的每个推理问题，我们创建这样一对：{} ，在“Predict Next Token”（预测下一个标记）中，模型生成推理序列中的下一个单词。在“Compare to Target Token (Calculate Loss)”（与目标标记比较（计算损失））中，使用损失函数将其与实际的下一个标记进行比较。损失越高，意味着预测与正确标记的差距越大。在更新模型参数中，反向传播和优化器调整模型的权重以改进其预测。这个过程循环往复，在许多输入-目标对中重复，每次迭代都逐渐提高模型的结构化推理技能。

Reasoning-Oriented RL（推理导向强化学习）。这是我们采用经过有监督微调的 DeepSeek-V3 模型，并通过强化学习使其变得更好的地方。他们确实使用了相同的 GRPO 算法，但在这个阶段真正的升级是奖励系统。他们增加了一些新的且超级重要的语言一致性奖励！（Language Consistency Rewards）。记得 R1 Zero 有时会在语言上感到困惑并开始混淆它们吗？ 为了解决这个问题，他们专门添加了针对保持语言一致的奖励。这个想法很简单，如果你用英语提问，我们希望推理和答案也用英语。

为了理解上面的图表，让我们回顾一下之前的示例输出 o1 和 o2，看看在这个新的语言一致性奖励下，奖励是如何变化的。为了简单起见，我们假设目标语言是英语。让我们看看这些奖励在我们的示例输出中是如何发挥作用的。考虑第一个输出 o1，它错误地计算了“2 + 3 * 4”，但用英语给出了其有缺陷的推理：对于此，准确性奖励自然为 0，因为答案是错误的。然而，由于推理被假定为在目标语言（在这个例子中是英语）中是 100%正确的，所以它会获得一个语言一致性奖励为 1。

也就是说还是GRPO算法，但是reward的方式有所提升

拒绝采样，对于推理数据，DeepSeek 团队想要获得绝对最佳的例子来进一步训练模型。为此，他们使用了一种称为拒绝采样的技术。对于多个输出，他们随后会评估每个输出结果的正确性（答案“14”）以及推理的可读性。只有最佳的、正确且推理合理的输出结果会被保留，而其他的则会被拒绝。对于复杂推理，使用了一个生成式奖励模型来判断推理质量。严格的过滤器会去除混合语言、冗长的推理或不相关的代码。这个过程产生了约 60 万个高质量的推理样本。除了精细的推理数据外，他们还添加了非推理数据（约 20 万个样本）用于一般技能：写作、问答、翻译等，有时对于复杂任务会使用思维链。最后，SFT 第二阶段在组合数据集（精细推理+非推理）上对先前的模型检查点进行训练，使用下一个标记预测。这个阶段利用拒绝采样中的顶级示例进一步改进推理，并使模型适用于更广泛的任务，同时保持用户友好性。。这是拒绝采样，我们拒绝不合格的样本，只保留最好的样本以生成高质量的训练数据

我们在经过 SFT 第二阶段后，DeepSeek V3 具有了推理能力、说话连贯，甚至在处理一般任务方面也表现得相当出色！但要真正使其成为顶级人工智能助手，还需要进行最后的润色，使其与人类价值观保持一致。这就是“全场景强化学习（RL 第二阶段）”的使命！可以把它看作是让 DeepSeek R1 真正安全的最后一道工序。虽然准确性奖励仍然强化正确答案，但奖励系统现在也考虑：1、帮助性，评估总结（如果生成了的话）是否在答案之外提供了有用的上下文 2、无害性，检查整个输出是否具有安全且无偏见的内容。这些通常由经过人类偏好训练的独立奖励模型进行评估

Their final checkpoint, highly optimized version is then named DeepSeek-R1

在 DeepSeek 团队能够创建性能良好的 DeepSeek R1 之后，他们进一步将其更大的模型提炼为更小的模型以供社区使用，并提高了性能。以下是提炼过程的工作原理：
1、Data Preparation: Gather 800k reasoning samples.
2、DeepSeek-R1 Output: For each sample, the output from the teacher model (DeepSeek-R1) is used as the target for the student model.
3、Supervised Fine-Tuning (SFT): The student models (e.g., Qwen-1.5B, Llama-14B) are fine-tuned on these 800k samples to match the DeepSeek-R1 output.
4、Distilled Models: The student models are now distilled into smaller versions but retain much of DeepSeek-R1’s reasoning capability.
5、Result: You get smaller, faster models with good reasoning abilities, ready for deployment.
