---
layout: post
title: llm
tags: [llm]
author-id: zqmalyssa
---

大语言模型

#### 基本概念

生成式的模型，chatgpt等语言模型本质还是像nlp的很多算法一样，预测下一个token，在解决幻觉的问题上（一本正经的胡说八道），会用到如rag的功能，打开搜索引擎也是

context engineering（上下文工程），就是确保输入的内容（input）足够的多，足够好，在chatgpt你的内容本质上会加上chattemplate，有些 system prompt 有些 user prompt。

声音（wavenet，取样点接龙）和图片（image-gpt，像素接龙）生成的本质也是next token（接龙游戏，autoregressive generation，自回归）。1024 * 1024像素图片的生成，要做100w次的接龙，声音也是（过于复杂了）。之后的就用了encoder/decoder的方法（vit等等），也就是神经网络/深度的压缩。

context engineering 和 prompt engineering 还是有一定区别的。context engineering是prompt 新的名字，原来的prompt方法不再神奇的时候（效果不大的时候），用了context，

之前gpt-3 或者 在gemini-1.5 有in-context learning（gpt-3论文就是给样例，gemini就是给整本翻译的教科书），claude opus 4.1中的system prompt特别长，有2516个word（交代的背景特别多，The assistant is Claude, created by anthropic..the current date is {{currentDateTime}}）

```html

if asked about the anthropic API, point to https://docs.anthropic.com

if user is unhappy, suggest pressing the thumbs down button # 回馈

claude does not provide info to make chemical or nuclear weapons # 安全

claude never starts its response with 'good question' # 风格

claude's knowledge cutof is Jan 2025 # 数据时间

```

dialogue history(短期记忆)，你是谁 这种经典的问题，能够记得对话的内容。

长期记忆(long-term memory)，经典的，我是什么样的人，开始对话的时候进行过询问（有一个设置），跨会话，结构化用户信息，或者向量化记忆（把对话中值得长期保存的片段变成向量，存入一个vectorDB），下次对话的时候使用找召回(retrieval) 把信息取回并注入prompt。

write的exp，再通过reflection，变成knowledge graph，再read回去。有些研究，比如graphRag和HippoRag，就是对模型下达指令后记下来，就会触发【已更新保存的记忆】，这是用chatgpt去做的，用了write组件

如果是工具使用上的话，也可以把所有的工具给存下来，再写一个tool selection就行了

reasoning部分的小剧场也算是context的内容，只是这部分是机器产生的，不是人制造的

这样的context里面的内容就很多，有user prompt、system prompt、长短期记忆、tool use、reasoning等等，是非常长的，而context engineering的核心目标就是避免塞爆context。只把需要的放进来，在AI Agent时代的话，context engineering很重要。workflow是一个sop，ai agent会更灵活，智能点，ai自己去选择做什么样的事。可以尝试使用gemini cli去做（一个免费的agent mode，帮你做各种事情）。如果agent要做的事情很复杂（写篇论文啥的），可能会导致输入（context）过于长了，模型可能会发疯了，没办法好好用。所以现在很多模型会有个【context window size】有多大，这个模型可以输入的长度上限。GPT-4可以放3w多个token，claude出来，可以放10w多个token。gemini-1.5说可以放100w个token。llama说可以放1000w个token。要模型一直运行，就要看很长的历史记录。200w的token是什么概念（哈利波特全集+指环王三部曲）。如果输入200w的token，模型是可以接收，但不能完全弄懂。可能还没到上限的时候，就感到困惑了。有论文说对于很长的context，模型比较记得开头和结尾，中间的会被忽略。。复述你的输入，各大语言模型会随着输入的越长，表现越差。

context engineering就是把不要的东西清出去，把要的东西放进来。大致有三招，select（rag，挑选正确的内容，细到挑选正确的句子，挑选工具，工具的说明要放到system_prompt，挑选记忆，对记忆做rag，把记忆放到另一个地方，必要的时候拿来用（时间越近、记忆越好，重要性，相关性等等）），compress（压缩之前的记忆，久远的就不要了，每互动100回合就压缩一次，），multi-agent（有个leader，出游，让agent1去订餐厅，让agent2去订旅馆这样，分散context，可以看langchain的研究）。

如果语言模型是一个f(x)，之前一直是对x进行一些设计，让模型更好的工作，现在看看f函数本身

输入文字后，经过tokenization，变成token id后，先和embedding table互动，embedding table是一个matrix，行就是vocabulary size（对应tokne的索引id）。列就是维度（向量，token embedding，值是一些数字，用作网络的输入）。第一层是考虑上下文的。可以叫经过第一层的embedding 为 (contextualized embedding)，或者(hidden / latent representation)。就是某一个layer的输出，最终一个矩阵（乘上最后一个向量），就是LM的head，输出vocabulary size的概率，但是乘上的LM Head得到的最终值是R值，官方称为logit，还不是概率，要做一个softmax。softmax就是转概率，当然也可以有其他的方式，一个变种就是把数值除以大T就再做指数运算，这个大T就是【temperature】，这个大T如果数字越大，那么做完softmax后，概率分布会越平均，数字越小，概率分布就会越集中（！！！！！这就是api中的temperature的底层逻辑，什么创意模式，保守模式就是通过控制这个大T达成的）。

unembedding，Llama，gemma等语言模型，LM head其实都不是独立的参数。直接把embedding table当做 LM head。？？？所以去计算LLM最终的layer输出的那个embedding跟每一个token embedding的相似度，就是 dot product，那概率高的，就是最终的向量与embedding table中向量相似度高的那个token了（这个过程可以理解成unembedding）。除了一样的token会有一样的token embedding外，相似的token也会有相似的token embedding（向量中的数字不是乱给的）。经过第一层后，同样的token得到的embedding在考虑了上下文的情况后就不一样了。特定的方向有特定的含义，比如某个方向代表中英翻译（cold -> 冷，hot -> 热，big -> 大）。这就是很多文献上man - woman 等于 king - queen的道理。分析embedding的方法可以是把高维的向量投影到低维的空间中。正常是有几千维的。某一层的representation抽出来，投影到某一个二维平面，这个平面需要自己找的？？。也可以修改representation，然后模型不停说脏话，也就说明这个representation代表的是说脏话方向。。（这个方向叫做representation engineering，activation engineering，activation steering）。所以可以操控语言模型 拒绝 或 同意你的请求。在某一层直接加上表示拒绝的向量后，模型就真的拒绝了。怎么知道在哪一层？：每一层都抽出拒绝的向量，看看哪一层会成功。(https://arxiv.org/abs/2406.11717，参考的是这篇论文)。反过来，如果减掉拒绝成分的话，一些违规的事情也可以做了。claude找到了谄媚的representation，回复会更加奉承。

还有分析LLM的方法就是logit lens。就是可以对每一层都做unembedding。每一层拿出来跟LM head乘乘 会得到什么东西，这个做法就叫做logit lens。还有的分析方法叫做patchscope，就是不只是一个单单的token，一句话了

当然一个layer里面就是网络架构了，比如transformer。会先有一个self-attention的layer，输入几个向量就输出几个向量。attention拿到了rnn的架构，只有attention是能够单独运作的。拿掉rnn后就可以做平行化了。训练模型更方便了。老样子，（q, k, v）。要有位置信息，所以加上了positional embedding。pe的话根据输入长度会变，那么llama中就用了Rope去处理位置。训练的时候只有1-4000，用的时候能1-8000。attention weight就是dot product算出来的分数，越大的话，会被attend到这个token。把attention weight乘上叫做value的vector。value vector是每个token再乘上W_v得到的。得到一个新的向量。本身的attention weight权重较低会不会忘了位置，不会、因为用了residual connection（残差连接），加回去了。不能说【两个青苹果】，青和果就有关系，两就和果没关系，所以一般是multi-head attention。找形容词的，找数量的等等，很多组attention weights。每个head会有自己的value vector。就是W_v_2。输入越长，运算量就越大。一般实践上只会考虑每个token左边（前面的token）。因为token是一个seq，这种只考虑前面的叫做causal attention。

#### 评估模型

evaluate一个模型。evaluate metric就是评估的标准，然后benchmark上对比evaluate metric的分数。需要定义一个小e算分数（小e的参数是模型输出和标准答案（ground truth）），小e的方法有exact match（选择题，选项固定，输出0-1，答案只能输出一个字母作为答案）

exact match有很多限制，所以用similarity去作为e，比对有多少共同的词汇，bleu（常用于翻译），rouge（常用于摘要）。但是 幽默 和 诙谐的比较又不能用共同词汇，所以就可以用embedding去做，得到contextualized embedding representation，这样虽然字面不一样，但是语义是相近的。用embedding 来计算相似度的可以参考bert score（https://arxiv.org/abs/1904.09675）

而过度相信evaluation的分数也是hallucination（幻觉）的原因之一，（https://arxiv.org/abs/2509.04664）。为什么模型不能在合适的时候说【我不知道】，因为这个在evaluation的时候不是很受青睐。给我不知道一个0分，给回答错误负分，也是可以提高一下回答 我不知道的 概率的

在没有标准答案的时候如何评判一个模型的好坏？找人去评估，代价有点大，不如用LLM as a judge。可以训练一个非常好的verifier专门用来做评分。

另外除了内容的好坏，还有其他维度的考虑，比如速度，从输入到输出第一个token要等多久，每秒平均可以生成多少的token。输入、输出、深度思考每一个token收费多少，思考，有些模型会用大量的toke进行深度思考，是否划算。

prompt可能会对evaluation有影响。还有恶意的使用jailbreak attack （明明不应该做，绕着弯让它做，或者用道理说服）和 prompt injection attack（比如AI review，文章中非常小的一行字，给高分）。 AI Agent Attack

#### 机器学习与深度学习

机器学习的基础，人类对任务的理解（domain knowledge）。看loss图的时候，假设只有w或者b，如果是3D的图，固定w，或者固定b，做个切面，就能看出另一个维度对loss的影响。3D图通常会画成loss的等高线图，loss surface。平面的等高线图也能看出哪个维度对loss影响比较大。常规的linear regression是有公式解的（closed-form solution）。小批量随机梯度下降算法（SGD），batch_size设置为1，在更新的时候是不稳定，左右来回，虽然优势是更新次数多。使用一个技术，shuffle，不要同样的数据在同一个batch里面，增加更多的随机性。模型训练出来还是需要先 验证（validation）再测试（Testing），在验证集上一算误差 1000多，在训练集上误差才200多，loss上升了5倍，结果非常糟糕。结果好的话当然可以直接去测试了。不好的话。

不好就要去推敲，训练/验证的（要解决的问题） 数据集画出来，假设是能在二维平面的，看看分布。完全不接近的话，换下训练的数据集。。还有个，参数搞出来的f不能拟合验证集的分布，假设是一个曲线，那么通过线性逼近曲线，叫做piecewise linear curve(分段线性曲线)。这样，二维上的任意曲线，就可以用式子cmax(0, w_1*x_1 + b)来表示。上面这个式子（不包括c）有个名字叫做rectified linear unit(ReLU)。这是另一种维度说的ReLU。如果从上面的式子看的话，deep l中如果有很多很多层，那么确实有机会能表示所有的函数。就算式子涵盖广后，loss还是没有降低，那么可能就是optimization做的不够好了。陷在local minimum，或者起始点就在比较平坦的位置，或者卡在收敛比较慢的地方，你觉得已经收敛了。

所以对于一个新的问题，可以先在线性模型上跑一下，如果loss在70多。。那么换成神经网络后，loss应该要低于70多才行。想办法改进超参数。。爆改，batch_size，lr，epoch啥的一通爆改。一通操作，可以把loss压到41，再往下压，是不是有更好的特征feature。但有时候 training的dataset 和 validation的 dataset差距太大了，就是overfitting了。当你学习的东西越大的时候，想涵盖一切表征的时候，就会容易出现overfitting。（考驾照的故事）。val的loss和train的loss可能在多个epoch会差距越来越大，所以把epoch设置小一点也是个好方法，叫做early stop。如果可以无限的用验证数据，又会产生验证的overfitting，同理，无限用测试，会在测试集上overfitting。benchmark的测试集，就算设置每天只能测一次，那么1000天，也是可以调出一个拟合测试集的模型的。通常把测试集分成public set 和 private set。private就只有一次机会了。

#### 训练技巧

可以按照 方法名 改了什么 带来什么好处 来看待这个问题，可以分成两类，better optimaization，更低的training loss 和 better generalization，更低的validation loss。

首先optimaization方向的，因为一般的学习率是争对所有参数的，所以是不是可以看看变化比较大的学习率就大，变化比较小的，学习率就小。方法adagrad，对每一个维度dimension（feature）,做梯度的平方的根号，就是绝对值sigema，然后用lr去除以这个值。然后到第二个迭代，把第一次和第二次两次的梯度，平方加和后开根号（虽然加和了，但是没有除以N），又得到新的sigema，再用lr去除以新的这个sigema。至于为什么没有除以N（其实也可以），是因为本身就是觉得lr应该越来越小，那么sigema其实就应该是越来越大的。后面的也是依次类推，把前面所有的平方加和后开根号。

如果使用adagrad，可以把lr设置的大一点，不然跑不动，因为lr会根据参数自动更新，所以不用在意初始的lr，可以设置大一点。核心思想，不同的参数gradient的大小不一样，lr也可以不一样

但adagrad在参数的grad有明显变化的时候，比如之前是[0.3, 0.2]等等量级的，后面变成[300, 200]量级的，不能再看之前的grad了，就产生了一个进阶版本，rmsprop。最近算出来的gradient给比较大的影响力，afa 和 1-afa去做一个权衡。

另一个想法是将momentum一个动量，或者冲量去加到optimaization，走到 small gradient 或者 saddle point的时候就有一个冲量让它继续走下去。就算走到local minimum，也可以让它有个动量冲出去。一开始momentum是历史的g的累积，然后可以结合rmsprop，控制下远近。注意不再按照grad的方向更新参数了，而是按照momentum的方向更新参数了。看例子，冲出了局部极小值

Adam：rmsprop + momentum。momentum过去的东西累加起来，是考虑方向的，rmsprop过去的东西的平方，其实没有考虑到方向，momentum会取代g，而rmsprop的sigema会控制lr。把这两件事情同时使用，就是adam了。可以作为default的optimaization了。2015年的文章，也就是同时累积平方梯度，累积动量。adam中还有一招，叫什么b？？（biased corrected 技术）

还有就是 learning rate scheduling，就是每次更新的时候，学习率不一样。现在LLM通常在某个次数之前是先增加再减少。lr减小叫做learning rate decay，让lr越来越大可以叫做warm up，给optimizer一个探索地形的机会。

总结，optimizer对训练是有帮助的，但是对generalization没啥帮助。也就是一上validation，直接趴菜。换optimizer 是想你要去压train loss但是压不下去。

对generalization有效果的方法是dropout，训练的时候丢到神经元，验证和测试的时候神经元又回来了。所以训练和验证的loss会比较接近。但是无法降低training loss，loss会比较高。使用这个技术的时机就是过拟合的时候。

不同的起始位置，可能会导致非常不错的训练结果。比如，在原来的初始位置乘上sqrt(2 / iput_dimension)。这是一个scale，何凯明提出来的。。initialization具体还有哪些好的地方不好说，但现在2025年有个通用的模式，叫做pre-train（self-supervised learning），然后就拥有了初始参数。pre-train会对optimaization和generalization都有帮助。

limu中有个权重衰减，也就是weight decay的方法，在loss上增加了一个惩罚项，就是w的L2 norm小于一个值sita（控制w的范围），小的sita意味着更强的正则项。但是优化的时候不会这么强硬的用这个 subject to，不好解这个问题，而是把惩罚项加到目标函数中，lambda / 2 * w的L2 norm。这个可以通过拉格朗日乘子去证明（跟强化学习PPO的那挂KL散度很像）。lambda等于0的时候就是惩罚项没什么效果，也就是回到之前的目标函数，sita可以是无穷大，增大lambda就是增加惩罚项的力度。（lambda非负）。相当于之前的sita趋于0了，那么w也趋于0了。算梯度再代入梯度下降后，更新的式子在之前的w上乘上一个权重，一般是小于1的，所以叫做权重衰减了

PPO中自适应散度的KL的beta，和正则化中的lambda一样，需要设置，先看KL中的解释，不想sita和sita_fi差距太大，我们先设一个可以接受的 KL 散度的最大值。假设优化完式后，KL 散度的值太大（大于最大值），这就代表后面惩罚的项没有发挥作用，我们就把beta的值增大。另外，我们设一个 KL 散度的最小值，如果优化完后，KL 散度比最小值还要小，就代表后面这一项的效果太强了（sita 和 sita_fi很像），所以我们要减小beta。式子里面体现了一个自适应（beta是变化的，而正则中的lambda是固定的），而且PPO应该是要最大化目标函数把，所以是减项，权重衰减是要最小化目标函数，所以是+项？按这么说，lambda也可以动态一下。对于最大化的目标，减一个东西，我们当然希望被减的东西越小越好。而对于最小化的目标，加一个东西，我们也希望加的东西越小越好，那么w就是要变小么（这样理解也是自然一点，limu画过一个图，一个是原始的l，一个是惩罚项L2 norm，原始的极值点位置因为惩罚项要往惩罚项的极值点跑（拉），这样惩罚项的值会变小，而l会增大，慢慢的两者达到一个平衡，同样的道理可以解释下PPO的惩罚项）


第二大类目，搞一个更好的网络。有一个不大的函数，但又能包含好的函数部分。可能需要凭借你的领域知识了（domain knownledge）。假设是一个CNN的结构。一张 1000 * 1000像素的RGB图片，第一层有1000个神经元，那么第一层的参数就是 1000 * 1000 * 3 * 1000 个参数，也就是3B，直接炸了，现在的小模型。人类去划一块 receptive field，（感受野），方块块。大小就叫做kernel size。这个receptive field是立体的啊，RGB三个channel都穿过的。如果是一个 2 * 2的kernal size原来的 1000 * 1000 会除以 500 * 500。大幅减少参数的使用量（这边是不是有问题。。。应该是把维度降到了通道上面）。参数共享的概念？？这边跟limu有出入，可以看下。会有一个更好的泛化。

梯度消失（gradient vanishing）,其实也是数值稳定性，skip connection(residual connection)。就是加上自己本身（输入），这个改变的是网络，但是带来的是optimaization的好处。

还有一整个系列的normalization方法。归一化。batch normalization 和 layer normalization。更好的去optimaization，可能有时候会有好的generalization。


第三大类目，loss，分类的cross-entropy对optimaization是有帮助的，但是对generalization有影响（因为你最终是要看分类的accuracy，而不是loss函数，有一个gap），那么又对optimaization 好，又对generalization好的是。。SVM里面的Hinge Loss。半监督学习中，式子后面有一个对于unlabel的entropy，就是对于没有分类的样本，让entropy较小，能够集中分布，而不是entropy则大，散乱。还有半监督的物以类聚，相连的看距离，不相连的不看距离，也是一种新的loss。半监督是更好的泛化，但不是optimaization。一般都喜欢比较好的平滑的线，不是波动比较大的，奥卡姆剃刀原则。。。这边可以说weight decay了（L2 regularization，解决的还是泛化的问题）

第四大类目，数据集，training data搜集到更多。overfitting的时候试试增加数据，实在收集不到数据就用data augmentation（数据增广）。旋转，模糊等等。


确实，看代码上的话，优化的方面可以往上面三个靠，网络，目标函数 和 优化算法。loss要能好好的算微分。然后带来的好处better optimaization 还是 better generalization

首先，优化算法类，adagrad、rmsprop、momentum、adam这些优化算法通常带来更好的optimaization，然后dropout和weight decay带来更好的泛化，更好的初始化同时带来optimaization和generalization

然后，设计的网络，残差网络（optimaization）、CNN（generalization）、normalization（optimaization）

最后，loss的改进，分类的cross-entropy，更多训练集，SVM，半监督。（都是为了更好的泛化）


#### 演化推进（训练大体方式）

2018 - 2019年是bert的时代，只有编码器。（芝麻街），要用起来就是在后面接【摘要】特别模型 或者 【翻译】特别模型

2020 - 2022年是gpt时代，就是解码器，gpt-3，要做【摘要】或者【翻译】需要微调模型，改变参数sita

2023 - 至今，算是比较通用了，long-time learning


目前2025左右

阶段1：pre-train（熟悉人类语言）

阶段2：SFT（）

阶段3：RLHF

后两个都可以当做alignment（对齐的一部分），SFT人类有标准答案，RLHF人类提供回馈。每个阶段可以上一个阶段模型的参数作为初始化。第一阶段训练数据，网络爬啊啥的，self-supervised learning。llama3用了15T的tokens去做训练，deepseek-v3用了 14.8trillion。huggingface 有 15t tokens的数据集，可以下载，用44TB的 disk能放叫，fine web。清理数据的论文（https://arxiv.org/abs/2406.11794），先用人为的一些filter做过滤，再用bloom filter dedup去重。gpt-3啊，palm的预训练效果都不好，问个问题，不好好回答，需要精雕一下，有人也认为pre-train的model还是很厉害的（https://arxiv.org/abs/2510.14901）。SFT 和 RL其实是帮助 pre-train的模型做出更正确的选择。SFT，用人类准备的数据（标注）来玩文字接龙。sft又叫做 instruction fine-tuning。无法准备大量的sft数据集。pre-train也能help到sft（数据量要多，对同一个问题，需要有不同的角度去看，举一反三）。SFT就是让模型好好的回答，不要乱回答。SFT阶段就不要教些新的东西，效果反而不好，死记硬背了，maybe known的数据反而能激发效果。alignment（sft + rlhf），往往没有给模型带来本质变化，改变的是结束符号的位置</s>。sft的路线有两条，1个是专才，（翻译、摘要、编程等等），1个是通才。前一种看看bert，也就是微调bert。（分类任务，标注任务，问答任务）。chatgpt，gemini这些走的是后一种（在sft的时候收集各种各样的数据，分类任务的、标注任务的、问答任务的）。SFT自己标注太累了，用现成的语言模型去弄，帮我们标注，同样的问题问老师（gpt）和学生，knowledge distillation 。降低了成本，cost不包含生数据和清数据的成本（老师生的时候要用api，filter也有人力成本），但是也还得准备问题，有没有可能准备问题都不要？（减少人工）。到RLHF部分，有赞和不赞的部分，是人类回馈，使用者没人给你写标准答案的。人类写不容易，但是判断答案好坏是比较容易的。还有个，reward是看一整个回答，不是看每个token间的loss和。还有，sft的时候可能给的数据模型本身就学不会，RL让模型学它本身要学的。RL一般是跟打游戏、下棋啥的有关，跟LLM有什么关系？？ 未完成的棋局是 RL中的 input，输出是下一步落子的位置。在LLM中，输入是未完成的句子，输出是下一个token。在语言模型得到输出的分布后再投一次骰子（？？？，就是说类似于env，），把输出再接到句子上，变成新的input。这样想RL的方法就能直接套用到RL上。之前RL的 PG内部有讲怎么算grad的，有一个非常完整的公式。PG类方法（PPO）在LLM中要干的事情就是 得到好的reward的时候 就让语言模型的输出 跟 正确答案拉近，人觉得不好的，式子上加一个负号，本来是拉近的，现在拉远？？？（有什么不光是要拉近，还需要拉远，。。什么意思）H是可以被替换成AI的，去训练一个reward model。。。

#### post-training 和 遗忘问题

post-training(把通用模型再打造成擅长某个领域的模型)，把post之前的叫做foundation model，把调完之后的叫做fine-tuned model。这个比之前提到的alignment需要的pre-train的model更广泛，foundation model可以是很多，chat的model也算的，已经做过alignment。post-training可以用上面说到的 阶段1-阶段2-阶段3的训练方式

比如LLaMA-2-Base在SFT和RLHF后已经具备了不错的能力，进化成了LLaMA-2-Chat，有了回答问题，安全啊等等能力，但是LLaMA-2-Base在pre-train的时候用的英文，需要它能回答中文，那就是做post-training，找很多中文的数据，进行后训练，这种后训练用了阶段1：pre-train style风格的方式。这样想模型不仅有之前的能力（回答能力），还能用中文回答。但是结果变成了能用中文回答，但是之前英文的能力没有了。这边可能会说是不是用的阶段1：pre-train的方法去试的，但是如果用阶段2（SFT）的话，其实也会导致之前的能力消失。比如安全问题，微调了一下改模型名字，消失了一点，用其他数据集微调了一下，安全问题能力直接消失。安全问题是因为做post-training后最容易被破坏的能力。

遗忘问题，catastrophic forgetting（灾难性遗忘）。

解决，以前，训练一个模型能力，想再训练一个能力，就可以把第一个能力的训练集混到第二个能力的训练集内。可以防止一点遗忘。那就可以拿一些训练LLaMA-2-Chat的数据混合在新的任务中，但是现在的模型的数据都不放出了。。就让模型自问自答 产生一些之前的训练数据，再加到新任务的数据中。那么之前安全问题的数据，也是可以产生出一点混到当下的训练中的。

还有的方法是对foundation model使用换句话说，也能产生训练集。用模型生成的答案，对于遗忘问题上，可能比人标注的数据要好。

#### 深度思考

像deepseek 这类大语言模型是怎么进行 【深度思考】，就是reasoning出来的流程，注意跟inference有区别，就要说到一个技术test time compute，这个在alphaGo等强化学习有早先的应用，alphaGo训练两个网络，policy network（决定下一步下那），value network（目前棋面的赢面）。test 的时候用mcts（monte carlo tree search）去找最好的选项，network policy的会提供很多的参考。有了test time compute 之后又有了 另一个词汇，叫做 test time scaling。思考越多往往结果越好。把树长的更深，多投点算力。

打造推理语言模型的方法，前两个方法不要微调参数，就能实现上面的深度思考，Cot，和 模型推论工作流程，后两个需要微调参数 教模型推理过程（imitation learning） 和 以结果为导向学习推理（reinforcement learning）

Cot，few-shot CoT（给一些范例），和 zero-shot Cot（let's think step by step）。这是2022年提出来的，现在的推理模型在做的话就叫做long Cot，（https://arxiv.org/abs/2503.09567）。prompt写多一点也可以做long的思考（supervised CoT,https://arxiv.org/abs/2410.14198）。第一种方法适用于比较强的模型

第二个方法，问模型一个问题几千几万次（https://arxiv.org/abs/2407.21787 large language monkeys），总有一次能回答正确，你如何知道哪个答案是正确的呢？majority vote（self-consistency）或者是confidence(used in Cot decoding)，概率。也可以让一个模型当verifier，当验证器（best of n），也可以加以训练，有training data，可以训练处一个专门的verifier。很多在中间步骤就进行验证，缩小思考的过程，错了就别思考了（process verifier）。如何决定切的阈值（beam search），每次保留N条路径（或者前25% 路径）。用beam search后，1B的模型就超越了8B的模型（用beam search的workflow）。可以把beam search换成mcts，用在LLM的推理过程中

第三个方法（imitation learning，直接教模型怎么做推理），需要微调，post-training， 从without reasoning（foundation model） 到 with reasoning（fine-tuned model）。training data里面不只有输出，还有对应的reasoning process（推理过程）。推理过程怎么来？？让语言模型自己想办法搞出来（cot），反正你是有标签的，找对应的。也可以找现成的语言模型去verifier。也可以结合RL中的reward机制去给出比较好的路径，tree struct。如果所有的推论step都是对的也不好，要找找自己的问题。DFS去找点不好的step，但是也不能直接就跳到另一个step，可以在两个step中间插入verifier的回馈，说哦哦，之前的推理错误了。"重新来过"。现在也可以做knowledge distillation（知识蒸馏），用会reasoning的模型的reasoning process给你的模型用。DeepSeek里面有知识蒸馏，（https://arxiv.org/abs/2501.12948 原始论文），论文中对比的时候，下面是拿来做知识蒸馏的foundation model，（qwen，llama等），用ds做老师教他们怎么做reasoning，教完后模型的能力起飞了。

第四个方法（reinforcement learning），这个就是DS-R1系列的做法，也是需要微调的，让模型去reasoning，不管内容是啥，只要结果是对的就是positive reward。答案不对就是negative reward。就是完全不管推论的过程 DeepSeek-v3-base（foundation model） -> 完全用RL（accuracy as reward） -> 做出了DeepSeek-R1-Zero（跟我们用的R1不是一回事），还有个reward是format的reward，模型要产生fakeToken，这就是为什么会产生fakeToken。rl后一个majority vote的结果，产生16个答案，16个答案做majority vote可以得到红色的线。majority vote的方法可以和RL的方法结合的，可以在force training的时候用RL的方法调整参数，在inference（是reasoning的时候把，，）的时候用majority vote再一次强化模型的能力。aha moment。RL自行得到的。R1-zero的reasoning很难让你理解，看如何演变成R1，还是用了zero的reasoning process。但是还是耗费了人力，human annotation，去更改生成的reasoning process。当做training data，然后做第三个方法的 imitation learning。从DeepSeek-v3-base 到 Model A，也用了第一部分讲过的prompt方法去强迫某一个模型生出有reasoning process的data，也是用于训练model A的（thousands of examples）。Model A 会去再进一步做 RL，得到Model B（除了有正确率的reward，还有语言必须要用一样的，也是reward）。有模型B后，给一些通用的input，包括一些没有标准答案的任务，DS-V3当做verify，现在model B得到的答案是不是一个好的答案。用一些规则去掉reasoning中比较糟糕的部分，比如语言混淆，讲太长了，code block。然后再做imitation learning，从DeepSeek-v3-base开始，60w数据集，也用v3做self-output（20w的数据集）。数据混在一起，得到Model C。再用Model C去做 RL 得到 R1，补充了safety 和 helpfulness的能力。32B的模型（qwen）很难用Rl强化，而V3-base有500B，RL的效果比较吃基础模型。RL能强化基础模型，说明原始模型可能本来就有reasoning的能力了，再用正确的答案作为奖励，强化

而reasoning太长了，不要做无谓的推断，reasoning越长的时候不代表回答的正确率越高。长回答就是额外算力的消耗。如何避免模型想太多呢？

对上面第一个方法，cot可以使用 chain of draft，就是 think step by step, but only keep a minimum draft for each thinking step

对上面第二个方法，比如做beam search的时候数小一点，控制模型reasoning的长度

对上面第三个方法，蒸馏的时候，可以问老师很多次，在答对的情况下，选择一个推理最少的作为训练样本，学生模型再拿训练数据学习

对上面第四个方法，ds单纯靠RL学习出来的模型reasoning真的非常冗长，DeepSeek-R1-Zero，直观是把长度的限制加到RL的reward中，但是对于每一种问题，不可能有一个统一的阈值。用相对的标准，答对的话比mean需要的推论长度还小就是好的。另外你问的时候设定一个推理长度，reward就定义成正确率-目标和实际推理长度的差异，差距非常小的话，reward才高。

#### model editing

model editing想要做的是帮模型植入一项知识。与post-training不同的是，post-training是要学会一项技能，比较大的改变，而model editing就是一笔训练数据，如果视为post-training，就是一个很特别的post-training。model editing的评估方法，reliability, generalization, locality。例子，假设谁是最美的人，答案 zqmalyssa，那么reliability保证回答正确，generalization保证问最美的人是谁的时候，也正确，locality就是不影响之前模型的性能。

一种是不改变参数，也是给些范例few-shot

一种是改变参数，1、是人类决定如何编辑，（rome），找出要编辑的地方，2、是人工智能决定如何编辑，有一个编辑模型，输出一个跟原模型参数一样多的e，再加回去。（hypernetwork）

#### model merging（比较新的东西，可以忽略）

不用任何训练数据，把别人的能力加过来，把模型的参数相减，，得出多余的能力。。（task vector），再直接加到需要加能力的model上。。22年的时候有人做了这方面的东西。。能使用的参数a 和 参数b 是从同一个foundation model fine-tune 出来的。这样是不是能解决串行post-training产生的遗忘问题呢。

反过来，减掉task vector的时候是不是就让模型失去某些能力

#### 开源的模型

llama、mistral、gemma，只是开源了参数，但是没有说模型是怎么被训练出来的
