---
layout: post
title: llm-practice
tags: [llm-practice]
author-id: zqmalyssa
---

æ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€LLMçš„ä¸€äº›å®è·µ

#### æä¸ªmodelä¸‹æ¥ï¼Œåšå®éªŒ

1ã€å¯ä»¥åœ¨hugging faceä¸Šmodelsé‡Œé¢æ‰¾å¼€æºçš„æ¨¡å‹ï¼Œåœ¨colabä¸Šè·‘ä¸€äº›å…è´¹çš„GPUï¼ŒGPUç›¸å…³çš„ï¼ŒGPT-3 è§„æ¨¡æ¨¡å‹è®­ç»ƒï¼šH100 æ¯” A100 å¿« 3â€“6 å€ã€‚å¤§æ¨¡å‹æ¨ç†ï¼ˆ70Bï¼‰ï¼šH100 æ¯” A100 å¿« 10â€“30 å€ï¼Œå°¤å…¶æ˜¯ FP8 è®­ç»ƒèƒ½åŠ›ï¼Œè®© H100 ä¸“ä¸ºå¤§æ¨¡å‹æ—¶ä»£è®¾è®¡ã€‚å…è´¹çš„ä¸€èˆ¬å°±æ˜¯T4ï¼ˆ4000-8000 äººæ°‘å¸ï¼‰ï¼Œç”¨T4è·‘ï¼ŒæŠŠ8Bçš„æ¨¡å‹æ¢æˆ1Bçš„æ¨¡å‹

NVIDIA A100 80GBï¼šçº¦ US$ 15,000â€“17,000 å·¦å³ã€‚
NVIDIA H100 80GBï¼šçº¦ US$ 25,000â€“30,000+ï¼Œé«˜ç«¯é…ç½®ï¼ˆå¦‚SXMç‰ˆï¼‰å¯èƒ½è¾¾åˆ° US$ 35,000â€“40,000

huggingface çš„ transformer æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œè§„é¿äº†åº•å±‚æ˜¯ç”¨pytorchè¿˜æ˜¯å•¥è®­ç»ƒçš„æ¨¡å‹ï¼Œè™½ç„¶å¯èƒ½æ²¡æœ‰ollemaæ•ˆç‡ï¼Œä½†å¼¹æ€§é«˜ã€å¹¿æ³›ä½¿ç”¨ã€‚æ‰€æœ‰ä¸Šä¼ åˆ°huggingfaceçš„æ¨¡å‹ï¼Œéƒ½èƒ½ç”¨å®ƒçš„transformeræ¡†æ¶æ‰§è¡Œã€‚è·å–huggingfaceçš„token

```html

from transformers import AutoTokenizer, AutoModelForCausalLM

# deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
# deepseek-ai/DeepSeek-R1-Distill-Llama-8B
# ç‰¹ä¹ˆä¸‹è½½ä¸€ä¸ªllama è¿˜è¦æˆæƒæ¥ç€ã€‚ã€‚è¯•è¯•ä¸Šé¢çš„ 1.5B å’Œ 8Bçš„ç‰ˆæœ¬ï¼Œä¸éœ€è¦æˆæƒï¼Œç›´æ¥ä¸‹è½½
model_id = "meta-llama/Llama-3.2-3B-Instruct"
#åªè¦æ›´æ› model ID å°±å¯ä»¥æ›æˆå…¶ä»–æ¨¡å‹äº†
#å‡è¨­ 3B æ¨¡å‹å¤ªå¤§ï¼Œä½ å¯èƒ½æœƒæƒ³è¦æ›æˆ 1B çš„æ¨¡å‹ (https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)
#ä½ åªéœ€è¦æŠŠä¸Šé¢çš„ "meta-llama/Llama-3.2-3B-Instruct" æ›æˆ "meta-llama/Llama-3.2-1B-Instruct" å³å¯
#æˆ–æ˜¯å¦‚æœä½ æƒ³è¦ç”¨ Google çš„ gemma (https://huggingface.co/google/gemma-3-4b-it)
#ä½ åªéœ€è¦æŠŠä¸Šé¢çš„ "meta-llama/Llama-3.2-3B-Instruct" æ›æˆ "google/gemma-3-4b-it" å³å¯
#ç¸½ä¹‹ï¼Œå¾ä»Šå¤©é–‹å§‹ï¼ŒHuggingFace ä¸Šçš„æ¨¡å‹éš¨ä¾¿ä½ ä½¿ç”¨ :)

# è®°å½•æ¨¡å‹æ‰€ä½¿ç”¨çš„tokenï¼Œvocabulary
tokenizer = AutoTokenizer.from_pretrained(model_id)
# æ¨¡å‹çš„å‚æ•°
model = AutoModelForCausalLM.from_pretrained(model_id)

```

add_special_tokens=False ä¼šä½¿å¾—å¥å­å‰é¢ä¸ä¼šåŠ ç‰¹æ®Štokenï¼Œç›´æ¥æ˜ å°„åˆ°tokenizerçš„ç´¢å¼•ï¼Œ

```html

# "good morning" å’Œ "i am good" ä¸­çš„ good ç·¨è™Ÿä¸€æ¨£å—ï¼Ÿç‚ºä»€éº¼ä¸ä¸€æ¨£ï¼Ÿ
print("good morning" ,"->", tokenizer.encode("good morning",add_special_tokens=False))
print("i am good" ,"->", tokenizer.encode("i am good",add_special_tokens=False))

# æ”¾åœ¨å¥é¦–å’Œå‰é¢æœ‰ä¸ªç©ºç™½ï¼Œgoodå¯¹åº”çš„tokençš„ç´¢å¼•ä¹Ÿä¸ä¸€æ ·

#æˆ‘å€‘ç”¨ tokenizer.encode æŠŠæ–‡å­—è®Šæˆä¸€ä¸² idï¼Œå†ç”¨ tokenizer.decode æŠŠ id è½‰å›æ–‡å­—

text = "å¤§å®¶å¥½"
# å°±æ˜¯ä¸€å®šåŠ ä¸€ä¸ªadd_special_tokens=Falseï¼Œä¸ç„¶ä¼šæœ‰begin_of_text
tokens = tokenizer.encode(text,add_special_tokens=False) #add_special_tokens=False å¯ä»¥é¿å…åŠ ä¸Šä»£è¡¨èµ·å§‹çš„ç¬¦è™Ÿ
text_after_encodedecode = tokenizer.decode(tokens)
print("åŸå§‹æ–‡å­—:",text)
print("ç·¨ç¢¼åœ¨è§£ç¢¼å¾Œ:",text_after_encodedecode)

```

ç°åœ¨å¼€å§‹ç”¨ä¸‹model

```html

import torch #æ¥ä¸‹ä¾†éœ€è¦ç”¨åˆ° torch é€™å€‹å¥—ä»¶

prompt = "1+1=" #è©¦è©¦çœ‹: "åœ¨äºŒé€²ä½ä¸­ï¼Œ1+1="ã€"ä½ æ˜¯èª°?"
print("è¼¸å…¥çš„ prompt æ˜¯:", prompt)

# model ä¸èƒ½ç›´æ¥è¼¸å…¥æ–‡å­—ï¼Œmodel åªèƒ½è¼¸å…¥ä»¥ PyTorch tensor æ ¼å¼å„²å­˜çš„ token IDs
# æŠŠè¦è¼¸å…¥ prompt è½‰æˆ model å¯ä»¥è™•ç†çš„æ ¼å¼
input_ids = tokenizer.encode(prompt, return_tensors="pt") # return_tensors="pt" è¡¨ç¤ºå›å‚³ PyTorch tensor æ ¼å¼
print("é€™æ˜¯ model å¯ä»¥è®€çš„è¼¸å…¥ï¼š",input_ids)

# model ä»¥ input_ids (æ ¹æ“š prompt ç”¢ç”Ÿ) ä½œç‚ºè¼¸å…¥ï¼Œç”¢ç”Ÿ outputsï¼Œ
outputs = model(input_ids)
# outputs è£¡é¢åŒ…å«äº†å¤§é‡çš„è³‡è¨Š
# æˆ‘å€‘åœ¨å¾€å¾Œçš„èª²ç¨‹é‚„æœƒçœ‹åˆ° outputs ä¸­é‚„æœ‰ç”šéº¼
# åœ¨é€™è£¡æˆ‘å€‘åªéœ€è¦ "æ ¹æ“šè¼¸å…¥çš„ prompt ï¼Œä¸‹ä¸€å€‹ token çš„æ©Ÿç‡åˆ†å¸ƒ" (ä¹Ÿå°±æ˜¯æ¯ä¸€å€‹ token æ¥åœ¨ prompt ä¹‹å¾Œçš„æ©Ÿç‡)

# outputs.logits æ˜¯æ¨¡å‹å°è¼¸å…¥æ¯å€‹ä½ç½®ã€æ¯å€‹ token çš„ä¿¡å¿ƒåˆ†æ•¸ï¼ˆé‚„æ²’è½‰æˆæ©Ÿç‡ï¼‰
# outputs.logits shape: (batch_size, sequence_length, vocab_size)
last_logits = outputs.logits[:, -1, :] #å¾—åˆ°ä¸€å€‹ token æ¥åœ¨ prompt å¾Œé¢çš„ä¿¡å¿ƒåˆ†æ•¸ (è‡³æ–¼ç‚ºä»€éº¼æ˜¯é€™æ¨£å¯«ï¼Œç•™çµ¦å„ä½åŒå­¸è‡ªå·±ç ”ç©¶)
probabilities = torch.softmax(last_logits, dim=-1) #softmax å¯ä»¥æŠŠåŸå§‹ä¿¡å¿ƒåˆ†æ•¸è½‰æ›æˆ 0~1 ä¹‹é–“çš„æ©Ÿç‡å€¼

# å°å‡ºæ©Ÿç‡æœ€é«˜çš„å‰ top_k å token
top_k = 10
top_p, top_indices = torch.topk(probabilities, top_k)
print(f"æ©Ÿç‡æœ€é«˜çš„å‰ {top_k} å token:")
for i in range(top_k):
    token_id = top_indices[0][i].item() # å–å¾—ç¬¬ i åçš„ token ID
    probability = top_p[0][i].item() # å°æ‡‰çš„æ©Ÿç‡
    token_str = tokenizer.decode(token_id) # å°‡ token ID è§£ç¢¼æˆæ–‡å­—
    print(f"Token ID: {token_id}, Token: '{token_str}', æ©Ÿç‡: {probability:.4f}")


```

ç”¨ä¸ŠchatTemplateï¼Œè®©æ¨¡å‹å¥½å¥½å›ç­”ï¼Œç­‰äºæŠŠ "AIç­”ï¼š" åŠ å…¥åˆ°ä½ çš„promptåé¢ï¼Œmodelåªæ˜¯ä¸‹ä¸€ä¸ªtokenï¼Œmodel.generateå°±å¯ä»¥å¥å­å‹çš„å›å¤äº†

```html

prompt = "ä½ æ˜¯èª°?"
print("ç¾åœ¨çš„ prompt æ˜¯:", prompt)
prompt_with_chat_template = "ä½¿ç”¨è€…èªªï¼š" + prompt + "\nAIå›ç­”ï¼š" #åŠ ä¸Šä¸€å€‹è‡ªå·±éš¨ä¾¿æƒ³çš„ Chat Template
print("å¯¦éš›ä¸Šæ¨¡å‹çœ‹åˆ°çš„ prompt æ˜¯:", prompt_with_chat_template)
input_ids = tokenizer.encode(prompt_with_chat_template, return_tensors="pt")

outputs = model.generate(
    input_ids,
    max_length=50,
    do_sample=True,
    top_k=3,
    pad_token_id=tokenizer.eos_token_id,
    attention_mask=torch.ones_like(input_ids)
)

# å°‡ç”¢ç”Ÿçš„ token ids è½‰å›æ–‡å­—
generated_text = tokenizer.decode(outputs[0]) # skip_special_tokens=True è·³éç‰¹æ®Š token

print("ç”Ÿæˆçš„æ–‡å­—æ˜¯ï¼š\n", generated_text)

#åŠ ä¸ŠChat Templateï¼Œèªè¨€æ¨¡å‹çªç„¶å¯ä»¥å°è©±äº†ï¼Œ æ¨¡å‹ä¸€ç›´æ˜¯åŒä¸€å€‹ï¼Œæ²’æœ‰æ”¹è®Šå–”!
#ä¸éé‚„æ˜¯æœ‰å•é¡Œï¼Œæ¨¡å‹å›ç­”å®Œå•é¡Œå¾Œï¼Œå¸¸å¸¸ç¹¼çºŒè‡ªå·±æå•ï¼Œé€™æ˜¯å› ç‚ºé€™è£¡çš„ Chat Template æ˜¯è‡ªå·±äº‚æƒ³çš„

```

ä¸Šé¢é‚£ç§è‡ªå·±åŠ Chat Templateçš„æ–¹æ³•ä¸ä¸€å®šå¯ä»¥çœ‹æ‡‚ï¼Œå°¾éƒ¨è¿˜æœ‰æ¥é¾™ï¼Œå¯ä»¥ä½¿ç”¨tokenizer.apply_chat_templateåŠ ä¸Šå®˜æ–¹çš„chat_templateã€‚

```html

prompt = "ä½ æ˜¯èª°?"
print("ç¾åœ¨çš„ prompt æ˜¯:", prompt)
messages = [
    {"role": "user", "content": prompt},
]
print("ç¾åœ¨çš„ messages æ˜¯:", messages)

input_ids = tokenizer.apply_chat_template(  #ä¸åªåŠ ä¸ŠChat Templateï¼Œé †ä¾¿å¹«ä½  encode äº†
    messages,
   add_generation_prompt=True,
    # add_generation_prompt=True è¡¨ç¤ºåœ¨æœ€å¾Œä¸€å€‹è¨Šæ¯å¾ŒåŠ ä¸Šä¸€å€‹ç‰¹æ®Šçš„ token (e.g., <|assistant|>)
   # é€™æœƒå‘Šè¨´æ¨¡å‹ç¾åœ¨è¼ªåˆ°å®ƒå›ç­”äº†ã€‚
    return_tensors="pt"
)


print("tokenizer.apply_chat_template çš„è¼¸å‡ºï¼š\n", input_ids)
print("===============================================\n")
print("ç”¨ tokenizer.decode è½‰å›æ–‡å­—ï¼š\n", tokenizer.decode(input_ids[0]))
print("===============================================\n")

### ä»¥ä¸‹ç¨‹å¼ç¢¼è·Ÿå‰ä¸€æ®µç¨‹å¼ç¢¼ç›¸åŒ ###

outputs = model.generate(
    input_ids,
    max_length=100,
    do_sample=True,
    top_k=3,
    pad_token_id=tokenizer.eos_token_id,
    attention_mask=torch.ones_like(input_ids)
)

# å°‡ç”¢ç”Ÿçš„ token ids è½‰å›æ–‡å­—
generated_text = tokenizer.decode(outputs[0])

print("ç”Ÿæˆçš„æ–‡å­—æ˜¯ï¼š\n", generated_text)

```

model å’Œ model.generate éƒ½ä¸éœ€è¦ä½¿ç”¨ gpuï¼Œtransformersä¸­çš„pipelineéœ€è¦ä½¿ç”¨gpuï¼Œç”Ÿæˆä¼šå˜å¾—å¾ˆæ…¢å¾ˆæ…¢ï¼Œpipelineçœç•¥æ‰å°†æ–‡å­—è½¬æˆtoken_id å†è½¬å›å»çš„è¿‡ç¨‹ã€‚

```html
from transformers import pipeline

# å»ºç«‹ä¸€å€‹pipelineï¼Œè¨­å®šè¦ä½¿ç”¨çš„æ¨¡å‹
# æ¢æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½
model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
#model_id = "google/gemma-3-4b-it"
pipe = pipeline(
    "text-generation",
   model_id
)

messages = [{"role": "system", "content": "ä½ æ˜¯ DeepSeekï¼Œä½ éƒ½ç”¨ä¸­æ–‡å›ç­”æˆ‘ï¼Œé–‹é ­éƒ½èªªå“ˆå“ˆå“ˆ"}]

while True:
    # 1ï¸âƒ£ ä½¿ç”¨è€…è¼¸å…¥è¨Šæ¯
    user_prompt = input("ğŸ˜Š ä½ èªªï¼š ")

    # å¦‚æœè¼¸å…¥ "exit" å°±è·³å‡ºèŠå¤©
    if user_prompt.lower() == "exit":
        #print("èŠå¤©çµæŸå•¦ï¼Œä¸‹æ¬¡å†èŠå–”ï¼ğŸ‘‹")
        break

    # å°‡ä½¿ç”¨è€…è¨Šæ¯åŠ é€²å°è©±ç´€éŒ„
    messages.append({"role": "user", "content": user_prompt})

    '''
    # 2ï¸âƒ£ å°‡æ­·å²è¨Šæ¯è½‰æ›ç‚ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼
    # add_generation_prompt=True æœƒåœ¨è¨Šæ¯å¾Œé¢åŠ å…¥ä¸€å€‹ç‰¹æ®Šæ¨™è¨˜ (<|assistant|>)ï¼Œ
    # å‘Šè¨´æ¨¡å‹ç¾åœ¨è¼ªåˆ°å®ƒè¬›è©±äº†ï¼
    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    )

    # 3ï¸âƒ£ ç”Ÿæˆæ¨¡å‹çš„å›è¦†
    outputs = model.generate(
        input_ids,
        max_length=2000, #é€™å€‹æ•¸å€¼éœ€è¦è¨­å®šå¤§ä¸€é»
        do_sample=True,
        top_k=10,
        pad_token_id=tokenizer.eos_token_id,
        attention_mask=torch.ones_like(input_ids)
    )

    # å°‡æ¨¡å‹çš„è¼¸å‡ºè½‰æ›ç‚ºæ–‡å­—
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)

    # ğŸ” å¾ç”Ÿæˆçµæœä¸­å–å‡ºæ¨¡å‹çœŸæ­£çš„å›è¦†å…§å®¹ï¼ˆå»é™¤ç‰¹æ®Štokenï¼‰
    # Llama æ¨¡å‹æœƒç”¨ç‰¹æ®Šçš„ token å€éš”è¨Šæ¯é ­å°¾ï¼Œæ ¼å¼é€šå¸¸æ˜¯é€™æ¨£çš„ï¼š
    # [è¨Šæ¯é ­éƒ¨]<|end_header_id|> æ¨¡å‹çš„å›è¦†å…§å®¹ <|eot_id|>
    response = generated_text.split("<|end_header_id|>")[-1].split("<|eot_id|>")[0].strip()
    '''

    ### ä¸Šè¿°è¨»è§£ä¸­çš„ç¨‹å¼ç¢¼æ‰€åšçš„äº‹æƒ…ï¼Œå¯ä»¥åƒ…ç”¨ä»¥ä¸‹å¹¾è¡Œç¨‹å¼ç¢¼å®Œæˆã€‚
    #=============================
    outputs = pipe(  # å‘¼å«æ¨¡å‹ç”Ÿæˆå›æ‡‰
      messages,
      max_new_tokens=2000,
      pad_token_id=pipe.tokenizer.eos_token_id
    )
    response = outputs[0]["generated_text"][-1]['content'] # å¾è¼¸å‡ºå…§å®¹å–å‡ºæ¨¡å‹ç”Ÿæˆçš„å›æ‡‰
    #=============================

    # 4ï¸âƒ£ é¡¯ç¤ºæ¨¡å‹çš„å›è¦†
    print("ğŸ¤– åŠ©ç†èªªï¼š", response)

    # å°‡æ¨¡å‹å›è¦†åŠ é€²å°è©±ç´€éŒ„ï¼Œè®“ä¸‹æ¬¡æ¨¡å‹çŸ¥é“ä¹‹å‰çš„å°è©±å…§å®¹
    messages.append({"role": "assistant", "content": response})

```

å¦å¤–ï¼Œç”¨T4åŠ è½½ deepseek-ai/DeepSeek-R1-Distill-Llama-8Bï¼Œç›´æ¥OutOfMemoryErrorï¼Œæè¿°ï¼šOutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 548.12 MiB is free. Process 8165 has 14.20 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 129.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  

#### æ¨¡å‹åˆ©ç”¨RAGï¼Œä½¿ç”¨å·¥å…·

å¸¸è§„çš„ï¼Œè°ƒç”¨å‡½æ•°

è¿˜æœ‰çš„tool useï¼Œæ“æ§é¼ æ ‡å’Œé”®ç›˜ï¼ˆcomputer useï¼‰ï¼Œgptå¤„äºä»£ç†äººæ¨¡å¼ï¼Œå¯ä»¥è¯•è¯•çœ‹


#### çœ‹æ¨¡å‹å†…éƒ¨

çœ‹çœ‹æ¨¡å‹çš„å†…éƒ¨

```html

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")

model.num_parameters() #å¾—åˆ°å‚æ•°çš„è¾“å‡º

for name, param in model.named_parameters():
    print(f"{name:80}  |  shape: {tuple(param.shape)}")

#æ ¹æ“šè¼¸å‡ºï¼Œdeepseek æœ‰å¹¾å±¤å‘¢ï¼Ÿ

model.embed_tokens.weight                                                         |  shape: (151936, 1536)

embedding_tableï¼Œ151936çš„vocaæ•°é‡ï¼Œæ¯ä¸ª1536çš„ç»´åº¦ã€‚

å¯ä»¥çœ‹åˆ°ä¸€å…±28å±‚ã€‚

```

æœ€åå¯ä»¥çœ‹åˆ°ä¸€ä¸ªlm_head.weighï¼Œè¿™ä¸ªå°±è·Ÿllama-3bä¸ä¸€æ ·äº†ï¼Œllama-3bæ˜¯æ²¡æœ‰lm_head.weighçš„ï¼Œgemmaå‰é¢çš„ä¸€äº›å‚æ•°æ˜¯ç”¨æ¥å¤„ç†å›¾ç‰‡çš„ã€‚attentionå±‚éƒ½æœ‰q,k,v,oã€‚

```html
# å¯ä»¥æŠŠembedding tableæ‹¿å‡ºæ¥çœ‹çœ‹ã€‚

input_embedding = model.state_dict()["model.embed_tokens.weight"].numpy()

```

å¯ä»¥çœ‹çœ‹è·Ÿembeddingæœ€åƒçš„å…¶ä»–tokenï¼Œæ¯”å¦‚appleï¼ˆè‹¹æœï¼‰

```html

top_k = 20 #è‡ªå·±è¨­å®šä¸€å€‹æ•¸å€¼

# 1ï¸âƒ£ è®“ä½¿ç”¨è€…è¼¸å…¥ä¸€å€‹ token
token = input('è«‹è¼¸å…¥ä¸€å€‹ tokenï¼š') #è¼¸å…¥: apple, Apple, æ, ç‹ ç­‰ç­‰

# 2ï¸âƒ£ è½‰æ›æˆ token ID
token_id = tokenizer.encode(token)[1]
# ç‚ºä»€éº¼æ˜¯ [1]ï¼Ÿ
# tokenizer.encode() å›å‚³çš„æ˜¯: [BOS_token_id, token_id ...]
# print (tokenizer.encode(token)) <- è·‘é€™ä¸€è¡Œè©¦è©¦çœ‹
# ç¬¬ä¸€å€‹å…ƒç´  [0] æ˜¯ç‰¹æ®Šèµ·å§‹ç¬¦è™Ÿ (BOS)ï¼Œ
# æˆ‘å€‘çœŸæ­£æƒ³è¦çš„æ˜¯è¼¸å…¥çš„é‚£å€‹ token æœ¬èº« â†’ æ‰€ä»¥å– index 1
print("token id æ˜¯ ",token_id)

# 3ï¸âƒ£ å–å¾— token çš„ embedding
embbeding = [input_embedding[token_id]]

# 4ï¸âƒ£ è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
sims = cosine_similarity(embbeding, input_embedding)[0]

# 5ï¸âƒ£ æ’åºä¸¦å–æœ€ç›¸è¿‘ top_k,ä¸¦è¼¸å‡ºçµæœ
nearest = sims.argsort()[::-1][1: top_k+1] #æ’é™¤è‡ªå·±æœ¬èº«
print(f'å’Œ {token} æœ€ç›¸è¿‘çš„ {top_k} å€‹ tokenï¼š')
for idx in nearest:
  print(f'{tokenizer.decode(idx)} (score: {sims[idx]:.4f})')

```

ä¹Ÿå¯ä»¥çœ‹ä¸‹æ¯å±‚çš„representation

```html

inputs = tokenizer.encode("å¤§å®¶å¥½", return_tensors="pt")

print("ç·¨ç¢¼å¾Œçš„ Token IDsï¼š", inputs)

outputs = model(inputs, output_hidden_states=True) # output_hidden_states=True æ‰æœƒå›å‚³æ¯ä¸€å±¤çš„ representation (hidden states)

hidden_states = outputs.hidden_states
# hidden_states[0] -> embedding ï¼ˆæŠŠ token è½‰æˆ token embedding çš„çµæœ)
# hidden_states[1] ~ hidden_states[N] -> æ¯ä¸€å±¤ Transformer block çš„è¼¸å‡º
print(f"ä¸€å…±æ‹¿åˆ° {len(hidden_states)} å±¤ representationï¼ˆåŒ…å« token embeddingï¼‰ã€‚")

# åˆ—å‡ºæ¯å±¤è¼¸å‡ºçš„å½¢ç‹€
for idx, h in enumerate(hidden_states):
    print(f"Layer {idx:2d} è¼¸å‡ºå½¢ç‹€: {h.shape}")
    # h.shape = [batch_size, seq_len, hidden_size]
    # batch_size â†’ ä¸€æ¬¡è™•ç†çš„å¥å­æ•¸
    # sequence_length â†’ å¥å­è¢«åˆ‡æˆå¤šå°‘ token
    # hidden_size â†’ æ¯å€‹ token çš„å‘é‡é•·åº¦


print("\n=== Token Embedding è¼¸å‡º ===")
print(hidden_states[0])

print("\n=== ç¬¬ä¸€å€‹ Transformer Layer çš„è¼¸å‡º ===")
print(hidden_states[1])

```

#### åˆ©ç”¨å¤šå¼ GPUè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆè§£å†³å¤§æ¨¡å‹å†…å­˜ä¸è¶³é—®é¢˜çš„ä¸€äº›æ–¹æ³•ï¼‰

DeepSpeedã€Flash attentionã€liger kernel å’Œ quantization

å•ä¸ªGPUåŸºæœ¬è®­ç»ƒä¸é¸ŸLLMã€‚DeepSpeedå¯ä»¥æ›´å¥½çš„åˆ©ç”¨GPUè¿›è¡Œè®­ç»ƒï¼Œå¦‚æœè¾“å…¥éå¸¸é•¿ï¼Œç”¨Flash attention å’Œ liger kernelï¼Œå¦‚æœä½¿ç”¨colabä¸Šçš„GPUå»è·‘ï¼Œsmall vRAM interenceï¼Œå°±å¯ä»¥ç”¨åˆ°quantizationã€‚

å‡è®¾æ¨¡å‹æ˜¯8Bçš„ï¼Œå…‰æ¨¡å‹å­˜åˆ°GPUçš„è¯éœ€è¦ä¸‹é¢çš„å¤§å°ï¼Œé€šå¸¸ä¼šæŠŠfp32è½¬æˆ16fpã€‚å°äº†ä¹Ÿå¿«äº†ï¼Œå¤§éƒ¨åˆ†è‹±ä¼Ÿè¾¾çš„GPUå¯¹16æœ‰åŠ é€Ÿã€‚

fp32: 8B params = 8 * 10^9 * 32bit = 32GB
fp16: 8B params = 8 * 10^9 * 16bit = 16GB

ç¼©å°åï¼ŒLLMçš„weightsæ˜¯16GBï¼Œè®­ç»ƒååšåå‘ä¼ æ’­çš„gradä¹Ÿæ˜¯è¦å­˜çš„ï¼Œä¹Ÿæ˜¯16GBçš„è¯ï¼Œè€Œä½¿ç”¨adamç®—æ³•åï¼Œè¿˜è¦æœ‰momentum å’Œ varianceçš„å‚æ•°ï¼Œä½¿ç”¨åŸå§‹çš„32GBï¼Œå…¨éƒ¨åŠ èµ·æ¥å¤ªå¤§äº† 128GBã€‚

å¦å¤–ä¸€ä¸ªå¤§å°æ˜¯inputï¼Œå‡è®¾è¾“å…¥æ˜¯ 256 tokensï¼Œå¤§çš„è¯å¯èƒ½åˆ°16k tokensã€‚æ¨¡å‹æ¯ä¸€å±‚çš„è¾“å‡ºæ˜¯è¦å­˜ä¸‹æ¥çš„ï¼Œ8Bçš„æ¨¡å‹ä¼šæœ‰32å±‚ã€‚æ¯å±‚æœ‰attentionï¼Œffnnï¼Œlayer normç­‰ç­‰ï¼Œä¹Ÿå å¤§å°

activation recomputation (ä¹Ÿå°±æ˜¯grad checkpoint)ï¼Œæ„æ€å°±æ˜¯åœ¨forwardçš„æ—¶å€™ï¼Œä½ ä¸è¦æŠŠæ‰€æœ‰çš„å±‚è¾“å‡ºéƒ½å­˜ä¸‹æ¥ï¼Œåªå­˜ä¸€äº›é‡è¦çš„ã€‚backwardè¦ç”¨åˆ°çš„æ—¶å€™é‡æ–°ç®—ä¸€æ¬¡å°±è¡Œ

LLMçš„batch sizeé€šå¸¸å¾ˆå¤§ é€šå¸¸ä¼šæœ‰ 4-60M tokens per batchã€‚DeepSeek V3çš„batch size æ˜¯1920ï¼Œ32K contextï¼Œ61M tokensã€‚

gradient accumulationï¼Œå°±æ˜¯å†åˆ‡åˆ†ï¼Œ1920 = 16 * 120ï¼ŒæŠŠ16ä¸ªä¸¢è¿›å»æ›´æ–°ï¼Œå¾—åˆ°çš„gradå…ˆä¸æ›´æ–°ï¼Œç­‰åˆ°120ä¸ªè·‘å®Œç´¯åŠ åå†æ›´æ–°ã€‚

ä¼˜åŒ–åˆ†æˆ3ä¸ªéƒ¨åˆ†

1ã€paramsã€gradients å’Œ optimizer states

128GBä¸€ä¸ªGPUè£…ä¸ä¸‹å°±ç”¨å¤šä¸ªï¼Œæ³¨æ„æ˜¯æ¯ä¸ªç»„ä»¶éƒ½åˆ‡åˆ†åˆ°å¯¹åº”çš„GPUä¸­ï¼Œä¸»è¦è€ƒè™‘æ€ä¹ˆåˆ‡ï¼Œå¾®è½¯çš„å·¥å…·deepspeedï¼Œzero redundancy optimizerã€‚æœ‰zero-1ï¼ˆåˆ‡optimizer stateséƒ¨åˆ†ï¼Œæœ€å¤šçš„ï¼‰ã€zero-2ï¼ˆLLM 32 å’Œ gradï¼‰ã€zero-3ï¼ˆLLM 16ï¼‰ã€‚gpuä¹‹é—´ä¼ è¾“æ•°æ®çš„é€Ÿåº¦nvlinkï¼š 900GB/sæ˜¯å¾ˆå¿«çš„ã€‚

å‡è®¾ä¸€ä¸ª8Bçš„æ¨¡å‹ï¼Œç”¨çš„H100ï¼Œæ˜¯è®­ç»ƒä¸é¸Ÿçš„ï¼Œå¼€äº†zero-1ï¼Œzero-2ï¼Œzero-3åå°±å¯ä»¥è®­ç»ƒäº†ï¼Œåˆ‡åˆ†åçš„å ç”¨ç©ºé—´å‡å°‘

zero-offloadï¼Œå®åœ¨å¤ªå¤§ï¼Œè¿˜å¯ä»¥å…ˆæ”¾åˆ°cpuçš„ramã€‚åªè¦cpuçš„ramå¤Ÿå¤§éƒ½å¯ä»¥æ”¾è¿‡å»ã€‚ä½†æ˜¯GPUå’ŒCPUçš„ä¼ è¾“é€Ÿåº¦ä¼šéå¸¸æ…¢ï¼4å¼ A100ï¼Œä¸€å¼ 32GBã€‚ä¸ä¼šOOMã€‚å¤§è‡´ä¸Šçš„è¯ï¼Œ8Bçš„æ¨¡å‹ï¼Œ4å¼ A100çš„GPUèƒ½è·‘äº†ã€‚å¡å¤šçš„è¯èƒ½è®­ç»ƒçš„æ¨¡å‹å°±è¶Šå¤§äº†ã€‚

2ã€activations

attention è¦åœ¨GPUä¸Šè·‘çš„ï¼Œæ‰€ä»¥å¯ä»¥é‡å†™attentionçš„codeï¼Œè·‘åœ¨GPUä¸Šçš„functionã€‚

flash attention algorithmã€‚ faster training & less memory by optimized fetching from cpu ramã€‚ä»¥å‰spaceæ˜¯O(n^2)ï¼Œç°åœ¨å°±æ˜¯O(n)ã€‚qã€kã€v å’Œ A æ”¾åˆ°cpuä¸Šï¼Œä¹Ÿæ˜¯å‡å°å†…å­˜æ¶ˆè€—ã€‚flash attentionè¿™ä¸ªä¸œè¥¿åœ¨pytorchä¸Šæœ¬èº«ä¹Ÿæ”¯æŒäº†

å¦å¤–ä¸€ä¸ªæ˜¯ liger kernelï¼ŒåŠ è½½æ¨¡å‹çš„æ—¶å€™æ”¹ä¸‹æ–¹å¼

å¦‚æœè®­ç»ƒçš„æ¨¡å‹contextå¤ªé•¿ï¼Œæ˜¯ä¸ªè§†é¢‘å•¥çš„ï¼Œå°±å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„æ–¹æ³•

3ã€quantizationï¼ˆé‡åŒ–ï¼‰

lossy compression(æœ‰æŸå‹ç¼©)ï¼Œ 32bit quantization algorithm -> 8bit int4 -> dequantization algorithm -> 32bitï¼ˆè¿‘ä¼¼ï¼‰

å­˜å‚¨çš„æ—¶å€™ç”¨å‹ç¼©åçš„å­˜ï¼Œç®—æ³•æœ‰GGML familyã€GPTQã€AWQã€BitsAndBytesã€‚ä¸€ä¸ª8Bçš„æ¨¡å‹å‹ç¼©åˆ° 8B-Instruct-Q8_0ï¼Œ8bitçš„è¯ï¼Œåªéœ€è¦8GBå»è½½å…¥å°±è¡Œäº†ï¼Œcolabä¸Šå…è´¹çš„T4ã€15GBçš„å†…å­˜

#### æ¨¡å‹åŠ è½½

48GBçš„macï¼ŒDeepSeek-R1-Distill-Qwen-1.5B å’Œ DeepSeek-R1-Distill-Llama-8B å¯ä»¥åŠ è½½

DeepSeek-R1-Distill-Qwen-32B æ˜¯æ— æ³•æ­£å¸¸åŠ è½½çš„ï¼ŒåŠ è½½åˆ†ç‰‡checkpointæ—¶å€™è¢«OOM killerå¼ºåˆ¶æ€æ­»

```html

Loading checkpoint shards: 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8
Process finished with exit code 137 (SIGKILL)

32Bçš„å¤§æ¦‚ç‡æ˜¯å†…å­˜ä¸è¶³çš„

AutoModelForCausalLM.from_pretrainedçš„æ—¶å€™å¯ä»¥åŠ ä¸Š  

device_map="auto" # è®©macåŠ è½½çš„æ—¶å€™ä¸ä¸“é—¨èµ°cpuï¼Œè€Œæ˜¯mps

# è¿™ä¸ªéœ€è¦å®‰è£… pip install accelerate

# è¿™ä¸ªè¿˜æ²¡æœ‰åŠ é‡åŒ–ï¼Œå°±èƒ½è·‘èµ·æ¥äº†ï¼Œä½†æ˜¯.generate()ï¼Œæ—¶å€™inputIdsè¦å¸¦ä¸Šdevice

device = model.device

input_ids.to(device))

# è¿™æ ·çš„ç»“æœå°±æ˜¯æ¨ç†èµ·æ¥å¾ˆæ…¢ï¼Œ32Bçš„è·‘å®Œæ­¥å›æ¥è¿˜åœ¨æ¨ï¼Œè¯•è¯•åŠ ä¸Šé‡åŒ–ï¼Œä½†æ˜¯

# bitsandbytes-macos ä¸æ”¯æŒ 4bitã€8bit GPU åŠ é€Ÿï¼ŒMPS æ²¡æœ‰å¯¹åº”çš„ kernelï¼ŒMPS æ²¡æœ‰å¯¹åº”çš„ kernelï¼Œä»£ç ç›´æ¥æŠ¥é”™

# æ¯”è¾ƒå¥½çš„åšæ³•å°±æ˜¯ device_map="auto" + torch_dtype="float16"


```

macå®åœ¨ä¸è¡Œçš„è¯ï¼Œç›´æ¥ä½¿ç”¨mlxï¼Œå®˜æ–¹äº²å„¿å­ï¼Œä¸‹è½½é‡åŒ–æ¨¡å‹å°±è¡Œ

```html

# åšé‡åŒ–

mlx_lm.convert --hf-path /Users/zqmalyssa/Model/deepseek/DeepSeek-R1-Distill-Qwen-32B --mlx-path /Users/zqmalyssa/Model/mlx/DeepSeek-R1-Distill-Qwen-32B-4bit -q  --q-bits 4

```

#### æ¨¡å‹ç¼–è¾‘

#### æ¨¡å‹merge
